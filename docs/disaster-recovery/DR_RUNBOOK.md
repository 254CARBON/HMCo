# Disaster Recovery Runbook

**Validated**: October 21, 2025  
**Backup Solution**: Velero + MinIO  
**RTO**: 15 minutes  
**RPO**: 1 hour (hourly critical backups + daily full)

---

## DR Test Results âœ…

**Test Date**: October 21, 2025  
**Test Scenario**: Complete namespace deletion and restoration  
**Result**: âœ… **SUCCESS**

### What Was Tested

1. âœ… Created test namespace with multiple resource types
2. âœ… Backed up namespace (31 items)
3. âœ… Deleted entire namespace (simulated disaster)
4. âœ… Restored from backup
5. âœ… Verified all resources recovered:
   - Deployments (2/2 pods running)
   - Services (ClusterIP working)
   - ConfigMaps (data intact)
   - Secrets (credentials restored)
   - PersistentVolumeClaims (bound and accessible)

### Recovery Time

- **Backup Creation**: ~15 seconds
- **Namespace Deletion**: ~10 seconds
- **Restore Operation**: ~30 seconds
- **Pods Ready**: ~30 seconds
- **Total RTO**: **~90 seconds** âœ…

---

## Backup Infrastructure

### Current Configuration

**Velero Server**: Running in `velero` namespace  
**Node Agents**: 2 (one per node)  
**Storage Backend**: MinIO (bucket: `velero-backups`)  
**Backup Schedules**:
- **Daily**: 2 AM UTC, 30-day retention
- **Hourly Critical**: Top of every hour, 7-day retention
- **Weekly**: Sunday 3 AM UTC, 90-day retention

### Backup Scope

**Included Namespaces** (automated):
- `data-platform` - All data services
- `monitoring` - Prometheus, Grafana, AlertManager
- `registry` - Harbor registry
- `cert-manager` - Certificates
- `cloudflare-tunnel` - Tunnel configuration
- Hourly schedule targets `data-platform` and `monitoring` for 1-hour RPO

**Excluded**:
- `kube-system` - System components (rebuilt on cluster init)
- `kube-public` - Public configurations
- `default` - Temporary resources

---

## Disaster Scenarios

### Scenario 1: Single Pod Failure

**Impact**: Low  
**Recovery**: Automatic (Kubernetes restarts pod)  
**Action**: Monitor only

```bash
# Check pod status
kubectl get pods -n data-platform

# View pod events
kubectl describe pod <pod-name> -n data-platform
```

### Scenario 2: Service Degradation

**Impact**: Medium  
**Recovery**: Rollback deployment  
**RTO**: 2 minutes

```bash
# Rollback to previous version
kubectl rollout undo deployment/<name> -n data-platform

# Check rollout status
kubectl rollout status deployment/<name> -n data-platform
```

### Scenario 3: Namespace Corruption

**Impact**: High  
**Recovery**: Restore from backup  
**RTO**: 5 minutes

```bash
# List available backups
~/bin/velero backup get

# Restore specific namespace
~/bin/velero restore create <restore-name> \
  --from-backup daily-backup-<date> \
  --include-namespaces data-platform

# Monitor restore
~/bin/velero restore describe <restore-name>
```

### Scenario 4: Complete Cluster Failure

**Impact**: Critical  
**Recovery**: Rebuild cluster + restore all  
**RTO**: 2-4 hours

**Steps**:
1. Provision new Kubernetes cluster (1-2 hours)
2. Install Velero and configure MinIO backend (15 minutes)
3. Restore all namespaces from latest backup (30 minutes)
4. Verify services (30 minutes)
5. Update DNS if IP changed (15 minutes)

---

## Recovery Procedures

### Quick Recovery (Namespace Level)

**Use when**: Namespace is corrupted or deleted

```bash
# Step 1: Identify latest backup
~/bin/velero backup get | grep Completed

# Step 2: Create restore
~/bin/velero restore create restore-$(date +%Y%m%d-%H%M%S) \
  --from-backup <backup-name> \
  --include-namespaces <namespace-name>

# Step 3: Monitor restore
~/bin/velero restore describe <restore-name>

# Step 4: Wait for completion
~/bin/velero restore logs <restore-name>

# Step 5: Verify pods are running
kubectl get pods -n <namespace-name>
```

**Automation**: `./scripts/velero-restore-validate.sh --schedule daily-backup --namespace <namespace-name> --wait` performs the same workflow and can map restores into scratch namespaces for rehearsal.

### Full Platform Recovery

**Use when**: Complete platform failure

```bash
# Step 1: Ensure Velero is installed and connected to MinIO
~/bin/velero version

# Step 2: List backups in storage
~/bin/velero backup get

# Step 3: Restore all critical namespaces
for ns in data-platform monitoring registry cert-manager cloudflare-tunnel; do
  echo "Restoring $ns..."
  ~/bin/velero restore create restore-$ns-$(date +%Y%m%d-%H%M%S) \
    --from-backup weekly-full-backup-<date> \
    --include-namespaces $ns \
    --wait
done

# Step 4: Verify all pods are running
kubectl get pods -A | grep -v Running

# Step 5: Test service access
curl -I https://portal.254carbon.com
```

**Template option**: export `BACKUP_NAME=<latest-weekly-backup>` and run `kubectl create -f <(envsubst < k8s/storage/velero-restore-full.yaml)` to replay the entire platform in a single restore object.

### Database-Only Recovery

**Use when**: Database corruption but pods are fine

```bash
# PostgreSQL restore from backup
kubectl exec -n data-platform postgres-shared-0 -- \
  pg_dumpall -U postgres > backup-$(date +%Y%m%d).sql

# Restore specific database
kubectl exec -n data-platform postgres-shared-0 -- \
  psql -U postgres -d datahub < backup-file.sql
```

---

## Manual Backup Creation

### On-Demand Backup (Before Major Changes)

```bash
# Backup specific namespace
~/bin/velero backup create pre-change-backup-$(date +%Y%m%d-%H%M%S) \
  --include-namespaces data-platform \
  --wait

# Backup with labels
~/bin/velero backup create app-backup-$(date +%Y%m%d) \
  --selector app=datahub \
  --wait

# Full cluster backup
~/bin/velero backup create full-cluster-$(date +%Y%m%d) \
  --exclude-namespaces kube-system,kube-public,kube-node-lease \
  --wait
```

### Verify Backup

```bash
# Check backup status
~/bin/velero backup describe <backup-name>

# Check backup logs
~/bin/velero backup logs <backup-name>

# Download backup for offsite storage
~/bin/velero backup download <backup-name>
```

---

## Recovery Validation Checklist

After any restore operation:

- [ ] **Pods Running**: `kubectl get pods -A | grep -v Running`
- [ ] **Services Responding**: Test each service URL
- [ ] **Data Integrity**: Query databases, check file contents
- [ ] **Configurations**: Verify ConfigMaps and Secrets
- [ ] **Persistent Volumes**: Check PVC bindings and data
- [ ] **Network Connectivity**: Test ingress and service mesh
- [ ] **Monitoring**: Verify Prometheus is scraping metrics
- [ ] **Logs**: Check Loki is receiving logs
- [ ] **Backups**: Ensure Velero can create new backups

---

## Production Backup Schedule

### Automated Schedules

**Daily Backup** (2 AM UTC):
```bash
~/bin/velero schedule get daily-backup
```
- Scope: All critical namespaces
- Retention: 30 days
- Estimated size: ~2-5 GB

**Weekly Full Backup** (Sunday 3 AM UTC):
```bash
~/bin/velero schedule get weekly-full-backup
```
- Scope: Complete platform
- Retention: 90 days
- Estimated size: ~5-10 GB

### Monitoring Backups

```bash
# Check recent backups
~/bin/velero backup get

# Check for failed backups
~/bin/velero backup get | grep -v Completed

# View backup details
~/bin/velero backup describe <backup-name>
```

---

## Offsite Backup Strategy

### Current: MinIO (On-Premise)

**Pros**:
- Fast backup/restore
- Complete control
- No egress costs

**Cons**:
- Same physical location as production
- Vulnerable to site-wide disasters

### Recommended: Add S3 Replication

```bash
# Configure MinIO to replicate to AWS S3
mc mirror minio-local/velero-backups s3-remote/254carbon-backups

# Or use Velero with multiple storage locations
~/bin/velero backup-location create aws-s3 \
  --provider aws \
  --bucket 254carbon-backups \
  --config region=us-east-1
```

---

## Critical Data Protection

### Databases

**PostgreSQL** (data-platform):
- Backed up via Velero PVC snapshots
- Also backed up via pg_dump in DolphinScheduler workflow

**MinIO Objects**:
- Backed up via Velero
- Consider replication to external S3

### Configurations

**Secrets**:
- Backed up and encrypted by Velero
- Store backup encryption key separately

**ConfigMaps**:
- Backed up via Velero
- Also version controlled in Git

---

## Recovery Time Objectives (RTO)

| Scenario | RTO Target | Actual (Tested) |
|----------|------------|-----------------|
| Single pod | 1 minute | Automatic |
| Service rollback | 2 minutes | < 2 minutes |
| Namespace restore | 10 minutes | **90 seconds** âœ… |
| Full cluster rebuild | 4 hours | Not tested |

## Recovery Point Objectives (RPO)

| Data Type | RPO Target | Current |
|-----------|------------|---------|
| Application config | 24 hours | 24 hours |
| Database data | 1 hour | 1 hour âœ… (hourly-critical) |
| Object storage | 24 hours | 1 hour âœ… (hourly-critical) |

*DolphinScheduler pg_dump workflow retains database export history for gap coverage.

---

## Testing Schedule

### Regular DR Drills

**Monthly** (First Sunday):
- Test namespace restore
- Verify all resources recover
- Document any issues
- Update runbook
- Automate via `./scripts/velero-restore-validate.sh --schedule daily-backup --namespace data-platform --wait --cleanup`

**Quarterly**:
- Full cluster rebuild test
- Test with older backups
- Verify backup retention policies
- Test offsite backup retrieval

**Annually**:
- Complete disaster simulation
- Full platform rebuild from scratch
- Update all documentation
- Train team on procedures

---

## Troubleshooting

### Restore Fails

```bash
# Check Velero logs
kubectl logs -n velero deployment/velero

# Check restore details
~/bin/velero restore describe <restore-name> --details

# Check for stuck resources
kubectl get events -n <namespace> --sort-by='.lastTimestamp'
```

### Backup Fails

```bash
# Check Velero server logs
kubectl logs -n velero deployment/velero | grep error

# Check MinIO connectivity
kubectl exec -n data-platform minio-0 -- mc admin info local

# Verify storage location
~/bin/velero backup-location get
```

### PVC Not Restored

```bash
# Check if node-agent is running
kubectl get pods -n velero | grep node-agent

# Verify PVC was included in backup
~/bin/velero backup describe <backup-name> | grep pvc

# Check PVC status after restore
kubectl describe pvc <pvc-name> -n <namespace>
```

---

## Success Metrics

### DR Test Results âœ…

- âœ… Backup created successfully (31 items)
- âœ… Namespace completely deleted (disaster simulated)
- âœ… Restore completed in 90 seconds
- âœ… All resources recovered (5/5)
- âœ… Secrets and ConfigMaps intact
- âœ… Deployments running (2/2 pods)
- âœ… PVCs bound and accessible

### Production Readiness

- âœ… Automated daily backups configured
- âœ… Automated weekly backups configured
- âœ… Restore procedures tested and validated
- âœ… Recovery time within targets (90s < 10min target)
- âœ… Documentation complete
- ðŸ“‹ Offsite backup recommended (not yet implemented)

---

## Next Steps

### Immediate
- [x] DR test completed successfully
- [x] Runbook documented
- [ ] Schedule first monthly DR drill

### Short-term (This Week)
- [ ] Configure offsite backup to S3
- [ ] Test database-specific restore procedures
- [ ] Create automated DR test job

### Medium-term (This Month)
- [ ] Implement backup monitoring alerts
- [ ] Configure backup encryption key rotation
- [ ] Test full cluster rebuild scenario

---

## Emergency Contacts

**Backup Issues**: Check Velero logs and MinIO status  
**Restore Issues**: Review this runbook and test logs  
**Questions**: Consult platform team

---

**Last Tested**: October 21, 2025  
**Test Result**: âœ… PASS  
**Next Test Due**: November 3, 2025 (First Sunday)
