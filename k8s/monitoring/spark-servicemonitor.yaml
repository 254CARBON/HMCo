# ServiceMonitor for Spark Operator Metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: spark-operator
  namespace: data-platform
  labels:
    app: spark
    component: monitoring
spec:
  selector:
    matchLabels:
      app: spark-operator
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
---
# ServiceMonitor for Spark History Server Metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: spark-history-server
  namespace: data-platform
  labels:
    app: spark
    component: monitoring
spec:
  selector:
    matchLabels:
      app: spark-history-server
  endpoints:
  - port: history-ui
    interval: 30s
    path: /metrics
---
# PrometheusRule for Spark Job Failures
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: spark-job-alerts
  namespace: data-platform
  labels:
    app: spark
    component: alerting
spec:
  groups:
  - name: spark.jobs
    interval: 30s
    rules:
    # Alert: Spark Application Failed
    - alert: SparkApplicationFailed
      expr: |
        increase(spark_app_status_failed_total[5m]) > 0
      for: 1m
      labels:
        severity: critical
        component: spark
      annotations:
        summary: "Spark Application Failed"
        description: "Spark application {{ $labels.app_id }} has failed. Check logs for details."
    
    # Alert: Spark Application Stuck
    - alert: SparkApplicationStuck
      expr: |
        (time() - spark_app_status_running) > 3600
      for: 5m
      labels:
        severity: warning
        component: spark
      annotations:
        summary: "Spark Application Running Too Long"
        description: "Spark application {{ $labels.app_id }} has been running for over 1 hour."
    
    # Alert: Spark Driver Pod Memory High
    - alert: SparkDriverMemoryHigh
      expr: |
        container_memory_usage_bytes{pod=~".*-driver"} / container_spec_memory_limit_bytes{pod=~".*-driver"} > 0.85
      for: 5m
      labels:
        severity: warning
        component: spark
      annotations:
        summary: "Spark Driver Memory Usage High"
        description: "Driver pod {{ $labels.pod }} memory usage is above 85%."
    
    # Alert: Spark Executor Pod Memory High
    - alert: SparkExecutorMemoryHigh
      expr: |
        container_memory_usage_bytes{pod=~".*-exec-.*"} / container_spec_memory_limit_bytes{pod=~".*-exec-.*"} > 0.85
      for: 5m
      labels:
        severity: warning
        component: spark
      annotations:
        summary: "Spark Executor Memory Usage High"
        description: "Executor pod {{ $labels.pod }} memory usage is above 85%."
    
    # Alert: Spark Operator Down
    - alert: SparkOperatorDown
      expr: |
        up{job="spark-operator"} == 0
      for: 2m
      labels:
        severity: critical
        component: spark
      annotations:
        summary: "Spark Operator Is Down"
        description: "Spark Operator has been unreachable for more than 2 minutes."
    
    # Alert: Spark History Server Down
    - alert: SparkHistoryServerDown
      expr: |
        up{job="spark-history-server"} == 0
      for: 2m
      labels:
        severity: warning
        component: spark
      annotations:
        summary: "Spark History Server Is Down"
        description: "Spark History Server has been unreachable for more than 2 minutes."
    
    # Alert: No Spark Applications Running
    - alert: NoSparkApplicationsRunning
      expr: |
        count(spark_app_status{status="running"}) == 0 and hour() >= 8 and hour() <= 20
      for: 10m
      labels:
        severity: info
        component: spark
      annotations:
        summary: "No Spark Applications Currently Running"
        description: "No active Spark applications detected during business hours."
---
# PrometheusRule for Spark Data Quality Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: deequ-quality-alerts
  namespace: data-platform
  labels:
    app: deequ
    component: alerting
spec:
  groups:
  - name: deequ.quality
    interval: 30s
    rules:
    # Alert: Data Quality Score Low
    - alert: DataQualityScoreLow
      expr: |
        deequ_quality_score < 80
      for: 5m
      labels:
        severity: warning
        component: deequ
      annotations:
        summary: "Data Quality Score Low"
        description: "Data quality score for {{ $labels.table }} is {{ $value }}, below acceptable threshold of 80."
    
    # Alert: Data Quality Score Critical
    - alert: DataQualityScoreCritical
      expr: |
        deequ_quality_score < 70
      for: 2m
      labels:
        severity: critical
        component: deequ
      annotations:
        summary: "Data Quality Score Critical"
        description: "Data quality score for {{ $labels.table }} is {{ $value }}, below critical threshold of 70."
    
    # Alert: Quality Check Failed
    - alert: QualityCheckFailed
      expr: |
        increase(deequ_check_failures_total[5m]) > 0
      for: 1m
      labels:
        severity: warning
        component: deequ
      annotations:
        summary: "Deequ Quality Check Failed"
        description: "Quality check {{ $labels.check_name }} for table {{ $labels.table }} has failed."
    
    # Alert: Data Anomaly Detected
    - alert: DataAnomalyDetected
      expr: |
        deequ_anomaly_detected > 0
      for: 2m
      labels:
        severity: warning
        component: deequ
      annotations:
        summary: "Data Anomaly Detected"
        description: "Anomaly detected in {{ $labels.table }}.{{ $labels.column }} - {{ $labels.anomaly_type }}"
---
# Recording Rules for Spark Metrics
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: spark-recording-rules
  namespace: data-platform
  labels:
    app: spark
    component: recording
spec:
  groups:
  - name: spark.recording
    interval: 30s
    rules:
    # Record: Spark Application Success Rate
    - record: spark:app:success_rate:5m
      expr: |
        rate(spark_app_status_successful_total[5m]) / 
        (rate(spark_app_status_successful_total[5m]) + rate(spark_app_status_failed_total[5m])) * 100
    
    # Record: Spark Average Job Duration
    - record: spark:job:avg_duration:5m
      expr: |
        avg(rate(spark_job_duration_seconds_sum[5m]) / rate(spark_job_duration_seconds_count[5m]))
    
    # Record: Spark Executor Count by Application
    - record: spark:app:executor_count:current
      expr: |
        spark_app_executor_count
    
    # Record: Average Executor Memory Usage
    - record: spark:executor:memory_usage_percent:5m
      expr: |
        avg(container_memory_usage_bytes{pod=~".*-exec-.*"} / 
            container_spec_memory_limit_bytes{pod=~".*-exec-.*"} * 100) by (job, pod)
---
# ScrapeConfig for Spark Jobs (if using Prometheus scrape config)
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-scrape-config
  namespace: data-platform
  labels:
    app: spark
    component: monitoring
data:
  spark-scrape.yaml: |
    # Scrape config for Spark applications with JMX exporter
    - job_name: 'spark-applications'
      metrics_path: '/metrics/prometheus'
      scrape_interval: 30s
      scrape_timeout: 10s
      static_configs:
      - targets:
        - 'localhost:4040'  # Spark Driver UI port
        - 'localhost:8081'  # Spark Executor ports
      relabel_configs:
      - source_labels: [__address__]
        target_label: instance
