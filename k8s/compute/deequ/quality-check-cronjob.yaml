# CronJob for Daily Data Quality Checks
apiVersion: batch/v1
kind: CronJob
metadata:
  name: deequ-daily-checks
  namespace: data-platform
  labels:
    app: deequ
    component: scheduler
spec:
  # Run daily at 2 AM UTC
  schedule: "0 2 * * *"
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 10
  concurrencyPolicy: Replace
  
  jobTemplate:
    spec:
      backoffLimit: 2
      ttlSecondsAfterFinished: 86400  # 24 hours
      
      template:
        metadata:
          labels:
            app: deequ
            component: quality-check
        spec:
          serviceAccountName: deequ
          restartPolicy: OnFailure
          
          containers:
          - name: quality-check-trigger
            image: curlimages/curl:latest
            command:
            - /bin/sh
            - -c
            - |
              # Create SparkApplication for quality checks
              kubectl apply -f - <<'EOF'
              apiVersion: sparkoperator.k8s.io/v1beta2
              kind: SparkApplication
              metadata:
                name: deequ-quality-check-$(date +%Y%m%d-%H%M%S)
                namespace: data-platform
              spec:
                type: Python
                mode: cluster
                image: apache/spark:3.5.0
                imagePullPolicy: IfNotPresent
                pythonVersion: "3"
                
                mainApplicationFile: s3a://spark-code/quality/deequ_quality_check.py
                
                arguments:
                - "--constraint-file"
                - "s3a://deequ-config/constraints.json"
                - "--output-database"
                - "monitoring"
                - "--output-table"
                - "deequ_quality_checks"
                - "--fail-on-critical"
                - "false"
                
                sparkVersion: "3.5.0"
                restartPolicy:
                  type: Never
                
                driver:
                  cores: 2
                  coreLimit: "2000m"
                  memory: "2g"
                  labels:
                    version: 3.5.0
                    spark-role: driver
                  serviceAccount: spark-app
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                  volumeMounts:
                  - name: spark-local-dir
                    mountPath: /opt/spark/work-dir
                  - name: deequ-config
                    mountPath: /opt/spark/conf/deequ
                
                executor:
                  cores: 2
                  instances: 2
                  memory: "2g"
                  labels:
                    version: 3.5.0
                    spark-role: executor
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                
                volumes:
                - name: spark-local-dir
                  emptyDir:
                    sizeLimit: 10Gi
                - name: deequ-config
                  configMap:
                    name: deequ-config
              EOF
              
              # Wait for job completion
              JOB_NAME=$(kubectl get sparkapplications -n data-platform -o jsonpath='{.items[-1].metadata.name}')
              kubectl wait --for=condition=Succeeded sparkapplication/$JOB_NAME -n data-platform --timeout=3600s || true
            
            env:
            - name: KUBECONFIG
              value: /var/run/secrets/kubernetes.io/serviceaccount/kubeconfig
---
# CronJob for Weekly Data Profiling
apiVersion: batch/v1
kind: CronJob
metadata:
  name: deequ-weekly-profiling
  namespace: data-platform
  labels:
    app: deequ
    component: scheduler
spec:
  # Run weekly on Sunday at 1 AM UTC
  schedule: "0 1 * * 0"
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 8
  concurrencyPolicy: Replace
  
  jobTemplate:
    spec:
      backoffLimit: 2
      ttlSecondsAfterFinished: 604800  # 7 days
      
      template:
        metadata:
          labels:
            app: deequ
            component: profiling
        spec:
          serviceAccountName: deequ
          restartPolicy: OnFailure
          
          containers:
          - name: profiling-trigger
            image: curlimages/curl:latest
            command:
            - /bin/sh
            - -c
            - |
              # Create SparkApplication for profiling
              kubectl apply -f - <<'EOF'
              apiVersion: sparkoperator.k8s.io/v1beta2
              kind: SparkApplication
              metadata:
                name: deequ-profiling-$(date +%Y%m%d)
                namespace: data-platform
              spec:
                type: Python
                mode: cluster
                image: apache/spark:3.5.0
                imagePullPolicy: IfNotPresent
                pythonVersion: "3"
                
                mainApplicationFile: s3a://spark-code/quality/deequ_profiling.py
                
                arguments:
                - "--profile-config"
                - "s3a://deequ-config/profiling-config.yaml"
                - "--output-database"
                - "monitoring"
                - "--output-table"
                - "deequ_profiles"
                
                sparkVersion: "3.5.0"
                restartPolicy:
                  type: Never
                
                driver:
                  cores: 2
                  coreLimit: "2000m"
                  memory: "3g"
                  labels:
                    version: 3.5.0
                    spark-role: driver
                  serviceAccount: spark-app
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                  volumeMounts:
                  - name: spark-local-dir
                    mountPath: /opt/spark/work-dir
                
                executor:
                  cores: 2
                  instances: 3
                  memory: "2g"
                  labels:
                    version: 3.5.0
                    spark-role: executor
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                
                volumes:
                - name: spark-local-dir
                  emptyDir:
                    sizeLimit: 15Gi
              EOF
            
            env:
            - name: KUBECONFIG
              value: /var/run/secrets/kubernetes.io/serviceaccount/kubeconfig
---
# CronJob for Anomaly Detection
apiVersion: batch/v1
kind: CronJob
metadata:
  name: deequ-anomaly-detection
  namespace: data-platform
  labels:
    app: deequ
    component: scheduler
spec:
  # Run every 6 hours
  schedule: "0 */6 * * *"
  successfulJobsHistoryLimit: 10
  failedJobsHistoryLimit: 20
  concurrencyPolicy: Replace
  
  jobTemplate:
    spec:
      backoffLimit: 1
      ttlSecondsAfterFinished: 21600  # 6 hours
      
      template:
        metadata:
          labels:
            app: deequ
            component: anomaly-detection
        spec:
          serviceAccountName: deequ
          restartPolicy: OnFailure
          
          containers:
          - name: anomaly-detection-trigger
            image: curlimages/curl:latest
            command:
            - /bin/sh
            - -c
            - |
              # Create SparkApplication for anomaly detection
              kubectl apply -f - <<'EOF'
              apiVersion: sparkoperator.k8s.io/v1beta2
              kind: SparkApplication
              metadata:
                name: deequ-anomaly-$(date +%Y%m%d-%H%M%S)
                namespace: data-platform
              spec:
                type: Python
                mode: cluster
                image: apache/spark:3.5.0
                imagePullPolicy: IfNotPresent
                pythonVersion: "3"
                
                mainApplicationFile: s3a://spark-code/quality/deequ_anomaly_detection.py
                
                arguments:
                - "--anomaly-config"
                - "s3a://deequ-config/anomaly-detection.yaml"
                - "--history-days"
                - "30"
                - "--output-database"
                - "monitoring"
                - "--output-table"
                - "deequ_anomalies"
                
                sparkVersion: "3.5.0"
                restartPolicy:
                  type: Never
                
                driver:
                  cores: 1
                  coreLimit: "1000m"
                  memory: "2g"
                  labels:
                    version: 3.5.0
                    spark-role: driver
                  serviceAccount: spark-app
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                  volumeMounts:
                  - name: spark-local-dir
                    mountPath: /opt/spark/work-dir
                
                executor:
                  cores: 1
                  instances: 2
                  memory: "1g"
                  labels:
                    version: 3.5.0
                    spark-role: executor
                  env:
                  - name: AWS_ACCESS_KEY_ID
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: access-key
                  - name: AWS_SECRET_ACCESS_KEY
                    valueFrom:
                      secretKeyRef:
                        name: minio-secret
                        key: secret-key
                
                volumes:
                - name: spark-local-dir
                  emptyDir:
                    sizeLimit: 5Gi
              EOF
            
            env:
            - name: KUBECONFIG
              value: /var/run/secrets/kubernetes.io/serviceaccount/kubeconfig
