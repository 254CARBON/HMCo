---
# ETL Frameworks & Templates for 254Carbon Platform
# Provides reusable patterns for Extract-Load, API ingestion, and data quality

---
# API Connector - Reusable for any REST API data source
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-connector-script
  namespace: data-platform
data:
  connector.py: |
    import requests
    import json
    import logging
    from datetime import datetime
    from typing import Dict, List, Any
    import os
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class APIConnector:
        def __init__(self, endpoint: str, auth_type: str = "none", **auth_params):
            self.endpoint = endpoint
            self.auth_type = auth_type
            self.auth_params = auth_params
            self.session = requests.Session()
            
        def get_headers(self) -> Dict:
            headers = {"Content-Type": "application/json"}
            
            if self.auth_type == "bearer":
                headers["Authorization"] = f"Bearer {self.auth_params.get('token')}"
            elif self.auth_type == "api_key":
                headers[self.auth_params.get('key_header', 'X-API-Key')] = self.auth_params.get('key')
            elif self.auth_type == "basic":
                import base64
                creds = base64.b64encode(
                    f"{self.auth_params.get('username')}:{self.auth_params.get('password')}".encode()
                ).decode()
                headers["Authorization"] = f"Basic {creds}"
                
            return headers
        
        def fetch(self, path: str, params: Dict = None, method: str = "GET") -> List[Dict]:
            url = f"{self.endpoint}/{path}"
            headers = self.get_headers()
            
            try:
                if method == "GET":
                    response = self.session.get(url, headers=headers, params=params, timeout=30)
                elif method == "POST":
                    response = self.session.post(url, headers=headers, json=params, timeout=30)
                
                response.raise_for_status()
                data = response.json()
                
                logger.info(f"Successfully fetched from {path}")
                return data if isinstance(data, list) else [data]
                
            except requests.RequestException as e:
                logger.error(f"API request failed: {str(e)}")
                raise
        
        def batch_fetch(self, path: str, batch_params: List[Dict]) -> List[Dict]:
            results = []
            for params in batch_params:
                results.extend(self.fetch(path, params))
            return results

---
# Data Quality Checker - For validation of extracted data
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-quality-checker
  namespace: data-platform
data:
  quality_checker.py: |
    import pandas as pd
    import logging
    from typing import Dict, List, Tuple
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class DataQualityChecker:
        def __init__(self, df: pd.DataFrame, name: str = "dataset"):
            self.df = df
            self.name = name
            self.checks = []
            
        def check_null_columns(self, columns: List[str]) -> Tuple[bool, str]:
            """Check for null values in critical columns"""
            for col in columns:
                null_count = self.df[col].isnull().sum()
                if null_count > 0:
                    msg = f"Column '{col}' has {null_count} null values"
                    logger.warning(msg)
                    self.checks.append(("null_check", col, False, msg))
                    return False, msg
            return True, "No nulls found in critical columns"
        
        def check_duplicates(self, columns: List[str]) -> Tuple[bool, str]:
            """Check for duplicate rows"""
            dup_count = self.df.duplicated(subset=columns).sum()
            if dup_count > 0:
                msg = f"Found {dup_count} duplicate rows"
                logger.warning(msg)
                self.checks.append(("duplicate_check", str(columns), False, msg))
                return False, msg
            return True, "No duplicates found"
        
        def check_row_count(self, min_rows: int) -> Tuple[bool, str]:
            """Check minimum row count"""
            if len(self.df) < min_rows:
                msg = f"Expected at least {min_rows} rows, got {len(self.df)}"
                logger.warning(msg)
                self.checks.append(("row_count_check", str(min_rows), False, msg))
                return False, msg
            return True, f"Row count {len(self.df)} is acceptable"
        
        def check_value_range(self, column: str, min_val, max_val) -> Tuple[bool, str]:
            """Check if values are within expected range"""
            out_of_range = ((self.df[column] < min_val) | (self.df[column] > max_val)).sum()
            if out_of_range > 0:
                msg = f"Column '{column}' has {out_of_range} values out of range [{min_val}, {max_val}]"
                self.checks.append(("range_check", column, False, msg))
                return False, msg
            return True, f"All values in '{column}' within range"
        
        def get_report(self) -> Dict:
            """Generate quality report"""
            return {
                "dataset": self.name,
                "total_rows": len(self.df),
                "total_columns": len(self.df.columns),
                "checks": self.checks,
                "all_passed": all(c[2] for c in self.checks)
            }

---
# ETL Database Extract-Load Template
# Used for extracting from external databases and loading to data lake
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etl-db-extract-template
  namespace: data-platform
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM UTC
  jobTemplate:
    spec:
      backoffLimit: 3
      template:
        spec:
          serviceAccountName: dolphinscheduler-worker
          containers:
          - name: etl-extract-load
            image: python:3.10-slim
            env:
            - name: DB_HOST
              valueFrom:
                secretKeyRef:
                  name: external-postgres-template
                  key: host
            - name: DB_PORT
              valueFrom:
                secretKeyRef:
                  name: external-postgres-template
                  key: port
            - name: DB_USER
              valueFrom:
                secretKeyRef:
                  name: external-postgres-template
                  key: username
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: external-postgres-template
                  key: password
            - name: DB_NAME
              valueFrom:
                secretKeyRef:
                  name: external-postgres-template
                  key: database
            command:
            - /bin/sh
            - -c
            - |
              pip install psycopg2-binary sqlalchemy pandas pyarrow -q
              python << 'PYTHON'
              import os
              import logging
              from sqlalchemy import create_engine
              import pandas as pd
              from datetime import datetime
              
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)
              
              # Build connection string
              db_url = f"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}"
              
              try:
                  # EXTRACT
                  logger.info("Connecting to external database...")
                  engine = create_engine(db_url)
                  
                  logger.info("Extracting data...")
                  query = "SELECT * FROM source_table WHERE updated_at > NOW() - INTERVAL '1 day'"
                  df = pd.read_sql(query, engine)
                  
                  # VALIDATE
                  logger.info(f"Extracted {len(df)} rows")
                  if len(df) == 0:
                      logger.warning("No data extracted")
                      exit(0)
                  
                  # TRANSFORM
                  df['extracted_at'] = datetime.now().isoformat()
                  df['extraction_source'] = 'external-postgres'
                  
                  # LOAD
                  logger.info("Loading to data lake...")
                  parquet_file = f"/tmp/extracted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
                  df.to_parquet(parquet_file, index=False)
                  
                  logger.info(f"Successfully extracted {len(df)} rows and loaded to {parquet_file}")
                  
              except Exception as e:
                  logger.error(f"ETL failed: {str(e)}")
                  exit(1)
              PYTHON
            resources:
              requests:
                cpu: "500m"
                memory: "512Mi"
              limits:
                cpu: "1000m"
                memory: "1Gi"
          restartPolicy: OnFailure

---
# ETL API Ingestion Template
# Used for pulling data from REST APIs
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etl-api-ingest-template
  namespace: data-platform
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      backoffLimit: 3
      template:
        spec:
          serviceAccountName: dolphinscheduler-worker
          containers:
          - name: api-ingest
            image: python:3.10-slim
            env:
            - name: API_ENDPOINT
              valueFrom:
                secretKeyRef:
                  name: external-api-credentials-template
                  key: api_endpoint
            - name: API_KEY
              valueFrom:
                secretKeyRef:
                  name: external-api-credentials-template
                  key: api_key
            command:
            - /bin/sh
            - -c
            - |
              pip install requests pandas pyarrow -q
              python << 'PYTHON'
              import os
              import requests
              import pandas as pd
              from datetime import datetime
              import logging
              
              logging.basicConfig(level=logging.INFO)
              logger = logging.getLogger(__name__)
              
              try:
                  # EXTRACT
                  logger.info(f"Fetching data from {os.environ['API_ENDPOINT']}")
                  headers = {"Authorization": f"Bearer {os.environ['API_KEY']}"}
                  response = requests.get(
                      f"{os.environ['API_ENDPOINT']}/data",
                      headers=headers,
                      timeout=30
                  )
                  response.raise_for_status()
                  
                  # VALIDATE & TRANSFORM
                  data = response.json()
                  df = pd.DataFrame(data if isinstance(data, list) else [data])
                  
                  logger.info(f"Fetched {len(df)} records")
                  if len(df) == 0:
                      logger.warning("No data retrieved from API")
                      exit(0)
                  
                  # Add metadata
                  df['fetched_at'] = datetime.now().isoformat()
                  df['source'] = os.environ['API_ENDPOINT']
                  
                  # LOAD
                  output_file = f"/tmp/api_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
                  df.to_parquet(output_file, index=False)
                  logger.info(f"Saved {len(df)} records to {output_file}")
                  
              except Exception as e:
                  logger.error(f"API ingestion failed: {str(e)}")
                  exit(1)
              PYTHON
            resources:
              requests:
                cpu: "250m"
                memory: "256Mi"
              limits:
                cpu: "500m"
                memory: "512Mi"
          restartPolicy: OnFailure
