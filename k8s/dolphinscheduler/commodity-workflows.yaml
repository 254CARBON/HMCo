# DolphinScheduler Workflows for Commodity Data Platform
# Automated daily batch jobs for market, economic, and weather data

---
# ConfigMap with Workflow Definitions
apiVersion: v1
kind: ConfigMap
metadata:
  name: dolphinscheduler-commodity-workflows
  namespace: data-platform
  labels:
    app: dolphinscheduler
    component: workflow-templates
data:
  # Daily Market Data Ingestion Workflow
  market-data-daily.json: |
    {
      "name": "Daily Market Data Ingestion",
      "description": "Collect daily market prices for commodities, electricity, LNG, natural gas",
      "scheduleStartTime": "2025-01-01 02:00:00",
      "scheduleEndTime": "2030-12-31 23:59:59",
      "crontab": "0 2 * * *",
      "scheduleWarningType": "FAILURE",
      "timeout": 3600,
      "tenantCode": "default",
      "processType": "DAG",
      "tasks": [
        {
          "name": "ingest_crude_oil_prices",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Run SeaTunnel connector for crude oil prices\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"energy-prices\",\n    \"commodity\": \"crude_oil\",\n    \"date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"Crude oil prices ingested successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300
        },
        {
          "name": "ingest_natural_gas_prices",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Run SeaTunnel connector for natural gas prices\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"energy-prices\",\n    \"commodity\": \"natural_gas\",\n    \"date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"Natural gas prices ingested successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300,
          "preTasks": ["ingest_crude_oil_prices"]
        },
        {
          "name": "ingest_electricity_prices",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Run SeaTunnel connector for electricity prices\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"energy-prices\",\n    \"commodity\": \"electricity\",\n    \"date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"Electricity prices ingested successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300,
          "preTasks": ["ingest_natural_gas_prices"]
        },
        {
          "name": "ingest_lng_prices",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Run SeaTunnel connector for LNG prices\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"energy-prices\",\n    \"commodity\": \"lng\",\n    \"date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"LNG prices ingested successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300,
          "preTasks": ["ingest_electricity_prices"]
        },
        {
          "name": "validate_market_data",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "SELECT COUNT(*) as record_count, MAX(price_date) as latest_date FROM commodity_data.energy_prices WHERE price_date = CURRENT_DATE - INTERVAL '1' DAY",
            "sqlType": "QUERY"
          },
          "timeout": 600,
          "preTasks": ["ingest_lng_prices"]
        },
        {
          "name": "send_success_notification",
          "type": "HTTP",
          "params": {
            "url": "http://dolphinscheduler-alert:50053/alert",
            "httpMethod": "POST",
            "httpParams": [
              {
                "prop": "message",
                "value": "Market data ingestion completed successfully for $(date +%Y-%m-%d)"
              }
            ]
          },
          "timeout": 300,
          "preTasks": ["validate_market_data"]
        }
      ]
    }

  # Economic Indicators Collection Workflow
  economic-indicators-daily.json: |
    {
      "name": "Daily Economic Indicators Collection",
      "description": "Collect economic indicators from FRED, World Bank, and other sources",
      "scheduleStartTime": "2025-01-01 03:00:00",
      "scheduleEndTime": "2030-12-31 23:59:59",
      "crontab": "0 3 * * *",
      "scheduleWarningType": "FAILURE",
      "timeout": 3600,
      "tenantCode": "default",
      "processType": "DAG",
      "tasks": [
        {
          "name": "ingest_fred_indicators",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Collect FRED economic indicators\nindicators=(\"DCOILWTICO\" \"DHHNGSP\" \"DPROPANEMBTX\" \"GASREGW\" \"ELECPRICE\")\nfor indicator in \"${indicators[@]}\"; do\n  echo \"Collecting $indicator from FRED...\"\n  curl -X POST http://seatunnel-service:8080/execute \\\n    -H 'Content-Type: application/json' \\\n    -d \"{\n      \\\"connector\\\": \\\"economic-data\\\",\n      \\\"source\\\": \\\"FRED\\\",\n      \\\"series_id\\\": \\\"$indicator\\\",\n      \\\"date\\\": \\\"$(date +%Y-%m-%d)\\\"\n    }\"\ndone\necho \"FRED indicators collected successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300
        },
        {
          "name": "ingest_world_bank_data",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Collect World Bank commodity price indicators\necho \"Collecting World Bank commodity prices...\"\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"economic-data\",\n    \"source\": \"WORLD_BANK\",\n    \"dataset\": \"commodity_prices\",\n    \"date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"World Bank data collected successfully\""
          },
          "timeout": 1800,
          "retryTimes": 3,
          "retryInterval": 300,
          "preTasks": ["ingest_fred_indicators"]
        },
        {
          "name": "aggregate_economic_data",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "INSERT INTO commodity_data.economic_summary SELECT indicator_code, observation_date, AVG(value) as avg_value, source, CURRENT_TIMESTAMP as computed_at FROM commodity_data.economic_indicators WHERE observation_date = CURRENT_DATE - INTERVAL '1' DAY GROUP BY indicator_code, observation_date, source",
            "sqlType": "NON_QUERY"
          },
          "timeout": 600,
          "preTasks": ["ingest_world_bank_data"]
        }
      ]
    }

  # Weather Data Collection Workflow
  weather-data-hourly.json: |
    {
      "name": "Hourly Weather Data Collection",
      "description": "Collect weather forecasts affecting commodity production and transportation",
      "scheduleStartTime": "2025-01-01 00:00:00",
      "scheduleEndTime": "2030-12-31 23:59:59",
      "crontab": "0 */4 * * *",
      "scheduleWarningType": "NONE",
      "timeout": 1800,
      "tenantCode": "default",
      "processType": "DAG",
      "tasks": [
        {
          "name": "ingest_us_weather_forecast",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Collect US weather forecasts for key commodity regions\nregions=(\"houston:LZK:48,65\" \"chicago:LOT:31,69\" \"newyork:OKX:33,37\")\nfor region in \"${regions[@]}\"; do\n  IFS=':' read -r city office grid <<< \"$region\"\n  echo \"Collecting weather for $city...\"\n  curl -X POST http://seatunnel-service:8080/execute \\\n    -H 'Content-Type: application/json' \\\n    -d \"{\n      \\\"connector\\\": \\\"weather-data\\\",\n      \\\"location\\\": \\\"$city\\\",\n      \\\"office\\\": \\\"$office\\\",\n      \\\"grid\\\": \\\"$grid\\\"\n    }\"\ndone\necho \"Weather data collected successfully\""
          },
          "timeout": 1200,
          "retryTimes": 2,
          "retryInterval": 180
        },
        {
          "name": "analyze_weather_impact",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "INSERT INTO commodity_data.weather_impact_analysis SELECT location, AVG(temperature) as avg_temp, AVG(precipitation_probability) as avg_precip_prob, forecast_time, 'COMPUTED' as analysis_type FROM commodity_data.weather_forecasts WHERE ingestion_date = CURRENT_DATE GROUP BY location, forecast_time",
            "sqlType": "NON_QUERY"
          },
          "timeout": 600,
          "preTasks": ["ingest_us_weather_forecast"]
        }
      ]
    }

  # Alternative Data Integration Workflow
  alternative-data-weekly.json: |
    {
      "name": "Weekly Alternative Data Integration",
      "description": "Process and integrate alternative data sources for commodity insights",
      "scheduleStartTime": "2025-01-01 04:00:00",
      "scheduleEndTime": "2030-12-31 23:59:59",
      "crontab": "0 4 * * 0",
      "scheduleWarningType": "FAILURE",
      "timeout": 7200,
      "tenantCode": "default",
      "processType": "DAG",
      "tasks": [
        {
          "name": "scan_alternative_data_sources",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Scan MinIO for new alternative data files\necho \"Scanning for new alternative data...\"\nmc alias set minio http://minio-service:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY\nmc ls --recursive minio/commodity-data/alternative/ | grep -E '\\.parquet$|\\.csv$' > /tmp/new_files.txt\ncat /tmp/new_files.txt\necho \"Found $(wc -l < /tmp/new_files.txt) files to process\""
          },
          "timeout": 600
        },
        {
          "name": "ingest_alternative_data",
          "type": "SHELL",
          "params": {
            "rawScript": "#!/bin/bash\nset -e\n# Ingest alternative data using SeaTunnel\necho \"Processing alternative data files...\"\ncurl -X POST http://seatunnel-service:8080/execute \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"connector\": \"alternative-data\",\n    \"process_date\": \"'$(date +%Y-%m-%d)'\"\n  }'\necho \"Alternative data ingestion completed\""
          },
          "timeout": 3600,
          "retryTimes": 2,
          "preTasks": ["scan_alternative_data_sources"]
        }
      ]
    }

  # Data Quality Validation Workflow
  data-quality-checks.json: |
    {
      "name": "Data Quality Validation",
      "description": "Validate data quality across all commodity datasets",
      "scheduleStartTime": "2025-01-01 06:00:00",
      "scheduleEndTime": "2030-12-31 23:59:59",
      "crontab": "0 6 * * *",
      "scheduleWarningType": "FAILURE",
      "timeout": 1800,
      "tenantCode": "default",
      "processType": "DAG",
      "tasks": [
        {
          "name": "check_data_freshness",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "SELECT table_name, MAX(ingestion_time) as latest_ingestion, CURRENT_TIMESTAMP - MAX(ingestion_time) as age_hours FROM (SELECT 'energy_prices' as table_name, MAX(ingestion_time) as ingestion_time FROM commodity_data.energy_prices UNION ALL SELECT 'economic_indicators', MAX(ingestion_time) FROM commodity_data.economic_indicators UNION ALL SELECT 'weather_forecasts', MAX(ingestion_time) FROM commodity_data.weather_forecasts) t GROUP BY table_name",
            "sqlType": "QUERY"
          },
          "timeout": 300
        },
        {
          "name": "check_data_completeness",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "SELECT 'energy_prices' as table_name, COUNT(*) as row_count, COUNT(DISTINCT price_date) as date_count FROM commodity_data.energy_prices WHERE price_date >= CURRENT_DATE - INTERVAL '7' DAY",
            "sqlType": "QUERY"
          },
          "timeout": 300,
          "preTasks": ["check_data_freshness"]
        },
        {
          "name": "check_data_validity",
          "type": "SQL",
          "params": {
            "type": "TRINO",
            "datasource": "trino-coordinator:8080",
            "sql": "SELECT COUNT(*) as invalid_records FROM commodity_data.energy_prices WHERE price IS NULL OR price < 0 OR price > 10000",
            "sqlType": "QUERY"
          },
          "timeout": 300,
          "preTasks": ["check_data_completeness"]
        }
      ]
    }

---
# Instructions ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: dolphinscheduler-workflow-instructions
  namespace: data-platform
data:
  README.md: |
    # DolphinScheduler Commodity Workflows
    
    ## Overview
    This package contains automated workflows for commodity data platform:
    
    1. **Daily Market Data Ingestion** - Runs at 2 AM UTC
       - Collects crude oil, natural gas, electricity, and LNG prices
       - Validates data completeness
       - Sends success notifications
    
    2. **Daily Economic Indicators** - Runs at 3 AM UTC
       - Collects FRED economic indicators
       - Integrates World Bank commodity price data
       - Aggregates and computes summary statistics
    
    3. **Hourly Weather Data** - Runs every 4 hours
       - Collects weather forecasts for key commodity regions
       - Analyzes weather impact on commodities
    
    4. **Weekly Alternative Data** - Runs Sundays at 4 AM UTC
       - Processes alternative data sources from MinIO
       - Integrates with main commodity datasets
    
    5. **Daily Data Quality Checks** - Runs at 6 AM UTC
       - Validates data freshness
       - Checks data completeness
       - Identifies invalid records
    
    ## Import Instructions
    
    1. Access DolphinScheduler UI: https://dolphinscheduler.254carbon.com
    2. Login with admin/admin credentials
    3. Create a new project: "Commodity Data Platform"
    4. Import each workflow JSON file from the ConfigMap
    5. Configure API keys in the Secret: seatunnel-api-keys
    6. Enable and start the workflows
    
    ## API Keys Required
    
    Update the `seatunnel-api-keys` secret with your API credentials:
    - FRED_API_KEY: Federal Reserve Economic Data
    - EIA_API_KEY: Energy Information Administration
    - NOAA_API_KEY: National Oceanic and Atmospheric Administration
    - ICE_API_KEY: Intercontinental Exchange
    - WORLD_BANK_API_KEY: World Bank Data
    
    ## Monitoring
    
    Monitor workflow execution:
    ```bash
    kubectl logs -n data-platform -l app=dolphinscheduler-master --tail=100
    kubectl logs -n data-platform -l app=dolphinscheduler-worker --tail=100
    ```
    
    Check Superset dashboards for data quality metrics and pipeline status.

