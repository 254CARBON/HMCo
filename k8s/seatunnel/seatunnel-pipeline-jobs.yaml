---
apiVersion: batch/v1
kind: Job
metadata:
  name: seatunnel-postgres-cdc-job
  namespace: data-platform
spec:
  template:
    spec:
      containers:
      - name: seatunnel
        image: apache/seatunnel:2.3.4
        command:
        - /opt/seatunnel/bin/seatunnel.sh
        - --config
        - /opt/seatunnel/config/postgres-cdc-to-iceberg.conf
        - --deploy-mode
        - client
        volumeMounts:
        - name: config-volume
          mountPath: /opt/seatunnel/config
        resources:
          requests:
            memory: "512Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: config-volume
        configMap:
          name: postgres-cdc-config
      restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-quality-monitoring
  namespace: data-platform
spec:
  schedule: "0 */6 * * *"  # Run every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: seatunnel
            image: apache/seatunnel:2.3.4
            command:
            - /opt/seatunnel/bin/seatunnel.sh
            - --config
            - /opt/seatunnel/config/data-quality-monitoring.conf
            - --deploy-mode
            - client
            volumeMounts:
            - name: config-volume
              mountPath: /opt/seatunnel/config
            resources:
              requests:
                memory: "512Mi"
                cpu: "200m"
              limits:
                memory: "1Gi"
                cpu: "500m"
          volumes:
          - name: config-volume
            configMap:
              name: data-quality-config
          restartPolicy: OnFailure
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-cdc-config
  namespace: data-platform
data:
  postgres-cdc-to-iceberg.conf: |
    # SeaTunnel Job: PostgreSQL CDC to Iceberg
    # Change Data Capture from PostgreSQL to Iceberg

    env {
      execution.parallelism = 2
      job.mode = "STREAMING"
      checkpoint.interval = 60000
    }

    source {
      Postgres-CDC {
        hostname = "postgres-shared-service"
        port = 5432
        username = "postgres"
        password = "datahub"
        database-names = ["datahub"]
        table-names = ["datahub.metadata_aspect"]
        result_table_name = "postgres_cdc_data"

        # CDC configuration
        slot.name = "seatunnel_postgres_slot"
        decoding.plugin.name = "pgoutput"
        startup.mode = "latest-offset"
        include-schema-change-events = true

        schema = {
          fields {
            urn = "string"
            aspect = "string"
            version = "bigint"
            metadata = "string"
            systemmetadata = "string"
            createdon = "timestamp"
            createdby = "string"
            createdfor = "string"
          }
        }
      }
    }

    transform {
      # Add data quality checks
      Filter {
        source_table_name = "postgres_cdc_data"
        result_table_name = "filtered_data"
        condition = "aspect IS NOT NULL AND urn IS NOT NULL"
      }

      # Validate JSON structure in metadata field
      JsonPath {
        source_table_name = "filtered_data"
        result_table_name = "validated_data"
        path = "$.metadata"
        schema_check = true
      }
    }

    sink {
      Iceberg {
        catalog_name = "rest"
        catalog_type = "rest"
        warehouse = "s3://iceberg-warehouse/"
        uri = "http://iceberg-rest-catalog:8181"
        database = "ods"
        table = "datahub_metadata_cdc"

        # S3/MinIO configuration
        s3.access-key-id = "minioadmin"
        s3.secret-access-key = "minioadmin123"
        s3.region = "us-east-1"
        s3.endpoint = "http://minio-service:9000"

        # Iceberg table configuration
        partition_by = ["aspect", "year(createdon)"]

        # Write options
        write_format = "parquet"
        compression_codec = "zstd"

        # Schema
        schema = {
          fields {
            urn = "string"
            aspect = "string"
            version = "bigint"
            metadata = "string"
            systemmetadata = "string"
            createdon = "timestamp"
            createdby = "string"
            createdfor = "string"
          }
        }
      }
    }
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-quality-config
  namespace: data-platform
data:
  data-quality-monitoring.conf: |
    # SeaTunnel Job: Data Quality Monitoring
    # Validates data quality across Iceberg tables and reports issues

    env {
      execution.parallelism = 1
      job.mode = "BATCH"
      checkpoint.interval = 300000
    }

    source {
      Iceberg {
        catalog_name = "rest"
        catalog_type = "rest"
        warehouse = "s3://iceberg-warehouse/"
        uri = "http://iceberg-rest-catalog:8181"

        # Read from multiple tables for quality checks
        sql = "
          SELECT
            'customers_cdc' as table_name,
            COUNT(*) as record_count,
            COUNT(DISTINCT id) as unique_ids,
            COUNT(CASE WHEN name IS NULL THEN 1 END) as null_names,
            COUNT(CASE WHEN email IS NULL THEN 1 END) as null_emails,
            MIN(created_at) as min_created_at,
            MAX(created_at) as max_created_at
          FROM ods.customers_cdc
          UNION ALL
          SELECT
            'events' as table_name,
            COUNT(*) as record_count,
            COUNT(DISTINCT event_id) as unique_ids,
            COUNT(CASE WHEN event_type IS NULL THEN 1 END) as null_event_types,
            COUNT(CASE WHEN event_timestamp IS NULL THEN 1 END) as null_timestamps,
            MIN(event_timestamp) as min_created_at,
            MAX(event_timestamp) as max_created_at
          FROM raw.events
        "
        result_table_name = "quality_metrics"

        # S3/MinIO configuration for reading
        s3.access-key-id = "minioadmin"
        s3.secret-access-key = "minioadmin123"
        s3.region = "us-east-1"
        s3.endpoint = "http://minio-service:9000"
      }
    }

    transform {
      # Calculate quality scores
      Calculate {
        source_table_name = "quality_metrics"
        result_table_name = "quality_scores"

        # Quality score calculation (0-100)
        # Based on null percentages and uniqueness
        expression = "
          CASE
            WHEN record_count = 0 THEN 0
            ELSE
              100 - (
                (null_names / record_count * 20) +
                (null_emails / record_count * 20) +
                (null_event_types / record_count * 20) +
                (null_timestamps / record_count * 20) +
                (CASE WHEN unique_ids < record_count * 0.9 THEN 20 ELSE 0 END)
              )
          END as quality_score
        "
      }

      # Filter for poor quality data
      Filter {
        source_table_name = "quality_scores"
        result_table_name = "quality_issues"
        condition = "quality_score < 80"
      }
    }

    sink {
      # Send quality issues to Kafka for alerting
      Kafka {
        bootstrap.servers = "kafka-service:9092"
        topic = "data-quality-alerts"
        format = "json"

        kafka_config = {
          acks = "all"
          retries = 3
          max.in.flight.requests.per.connection = 1
        }
      }

      # Also write to Iceberg for historical analysis
      Iceberg {
        catalog_name = "rest"
        catalog_type = "rest"
        warehouse = "s3://iceberg-warehouse/"
        uri = "http://iceberg-rest-catalog:8181"
        database = "monitoring"
        table = "data_quality_metrics"

        # S3/MinIO configuration
        s3.access-key-id = "minioadmin"
        s3.secret-access-key = "minioadmin123"
        s3.region = "us-east-1"
        s3.endpoint = "http://minio-service:9000"

        # Partition by date
        partition_by = ["table_name", "year(CURRENT_DATE())"]

        # Schema
        schema = {
          fields {
            table_name = "string"
            record_count = "bigint"
            unique_ids = "bigint"
            null_names = "bigint"
            null_emails = "bigint"
            null_event_types = "bigint"
            null_timestamps = "bigint"
            quality_score = "double"
            check_timestamp = "timestamp"
          }
        }
      }
    }
