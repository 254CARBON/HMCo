# Advanced Commodity Data Connectors
# Additional financial market data sources: AlphaVantage, Polygon.io, OpenFIGI, GIE, US Census

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: seatunnel-advanced-connectors
  namespace: data-platform
  labels:
    app: seatunnel
    component: data-ingestion
data:
  # AlphaVantage Stock & Commodity Data
  alphavantage-connector.conf: |
    env {
      job.mode = "BATCH"
      checkpoint.interval = 10000
    }
    
    source {
      Http {
        url = "https://www.alphavantage.co/query"
        method = "GET"
        params {
          function = "TIME_SERIES_DAILY"
          symbol = "CL=F"  # Crude Oil Futures
          apikey = "${ALPHAVANTAGE_API_KEY}"
          outputsize = "full"
          datatype = "json"
        }
        format = "json"
        schema = {
          fields {
            timestamp = string
            open = double
            high = double
            low = double
            close = double
            volume = long
          }
        }
      }
    }
    
    transform {
      Sql {
        query = "SELECT 
                   timestamp as trade_date,
                   'crude_oil_futures' as symbol,
                   open, high, low, close, volume,
                   'ALPHAVANTAGE' as source,
                   CURRENT_TIMESTAMP() as ingestion_time
                 FROM alphavantage_data"
      }
    }
    
    sink {
      Iceberg {
        catalog_name = "iceberg_catalog"
        catalog_uri = "http://iceberg-rest-catalog:8181"
        namespace = "commodity_data"
        table = "market_prices_intraday"
        schema_save_mode = "RECREATE_SCHEMA"
        data_save_mode = "APPEND_DATA"
      }
    }

  # Polygon.io Real-time Market Data
  polygon-connector.conf: |
    env {
      job.mode = "BATCH"
      checkpoint.interval = 10000
    }
    
    source {
      Http {
        url = "https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{from}/{to}"
        method = "GET"
        headers {
          Authorization = "Bearer ${POLYGON_API_KEY}"
        }
        format = "json"
        schema = {
          fields {
            ticker = string
            timestamp = long
            open = double
            high = double
            low = double
            close = double
            volume = long
            vwap = double
          }
        }
      }
    }
    
    transform {
      Sql {
        query = "SELECT 
                   ticker as symbol,
                   FROM_UNIXTIME(timestamp/1000) as trade_datetime,
                   open, high, low, close, volume, vwap,
                   'POLYGON' as source,
                   CURRENT_TIMESTAMP() as ingestion_time
                 FROM polygon_data"
      }
    }
    
    sink {
      Iceberg {
        catalog_name = "iceberg_catalog"
        catalog_uri = "http://iceberg-rest-catalog:8181"
        namespace = "commodity_data"
        table = "polygon_market_data"
        schema_save_mode = "RECREATE_SCHEMA"
        data_save_mode = "APPEND_DATA"
      }
    }

  # OpenFIGI Instrument Mapping
  openfigi-connector.conf: |
    env {
      job.mode = "BATCH"
    }
    
    source {
      Http {
        url = "https://api.openfigi.com/v3/mapping"
        method = "POST"
        headers {
          Content-Type = "application/json"
          X-OPENFIGI-APIKEY = "${OPENFIGI_API_KEY}"
        }
        body = "[{\"idType\": \"TICKER\", \"idValue\": \"CL\"}]"
        format = "json"
        schema = {
          fields {
            figi = string
            name = string
            ticker = string
            exchCode = string
            compositeFIGI = string
            securityType = string
            marketSector = string
          }
        }
      }
    }
    
    transform {
      Sql {
        query = "SELECT 
                   figi,
                   compositeFIGI as composite_figi,
                   ticker,
                   name as instrument_name,
                   securityType as security_type,
                   exchCode as exchange,
                   marketSector as market_sector,
                   'OPENFIGI' as source,
                   CURRENT_TIMESTAMP() as ingestion_time
                 FROM openfigi_data"
      }
    }
    
    sink {
      Iceberg {
        catalog_name = "iceberg_catalog"
        catalog_uri = "http://iceberg-rest-catalog:8181"
        namespace = "commodity_data"
        table = "instrument_mapping"
        schema_save_mode = "RECREATE_SCHEMA"
        data_save_mode = "APPEND_DATA"
      }
    }

  # GIE (Gas Infrastructure Europe) - AGSI/ALSI Storage Data
  gie-storage-connector.conf: |
    env {
      job.mode = "BATCH"
    }
    
    source {
      Http {
        url = "https://agsi.gie.eu/api"
        method = "GET"
        headers {
          x-key = "${GIE_API_KEY}"
        }
        params {
          country = "EU"
          from = "${START_DATE}"
          to = "${END_DATE}"
        }
        format = "json"
        schema = {
          fields {
            gasDayStart = string
            name = string
            code = string
            url = string
            gasInStorage = double
            consumption = double
            full = double
            injection = double
            withdrawal = double
            workingGasVolume = double
            injectionCapacity = double
            withdrawalCapacity = double
            trend = double
          }
        }
      }
    }
    
    transform {
      Sql {
        query = "SELECT 
                   gasDayStart as gas_day,
                   name as facility_name,
                   code as facility_code,
                   gasInStorage as gas_in_storage_gwh,
                   consumption as consumption_gwh,
                   full as storage_full_pct,
                   injection as injection_gwh,
                   withdrawal as withdrawal_gwh,
                   workingGasVolume as working_gas_volume_gwh,
                   trend,
                   'GIE_AGSI' as source,
                   CURRENT_TIMESTAMP() as ingestion_time
                 FROM gie_data"
      }
    }
    
    sink {
      Iceberg {
        catalog_name = "iceberg_catalog"
        catalog_uri = "http://iceberg-rest-catalog:8181"
        namespace = "commodity_data"
        table = "gas_storage_europe"
        schema_save_mode = "RECREATE_SCHEMA"
        data_save_mode = "APPEND_DATA"
      }
    }

  # US Census Economic Indicators
  census-connector.conf: |
    env {
      job.mode = "BATCH"
    }
    
    source {
      Http {
        url = "https://api.census.gov/data/timeseries/eits/resconst"
        method = "GET"
        params {
          get = "cell_value,data_type_code,time_slot_id,category_code"
          key = "${CENSUS_API_KEY}"
          time = "from 2020"
        }
        format = "json"
        schema = {
          fields {
            cell_value = string
            data_type_code = string
            time_slot_id = string
            category_code = string
          }
        }
      }
    }
    
    transform {
      Sql {
        query = "SELECT 
                   time_slot_id as time_period,
                   category_code,
                   data_type_code,
                   CAST(cell_value AS DOUBLE) as value,
                   'US_CENSUS' as source,
                   CURRENT_TIMESTAMP() as ingestion_time
                 FROM census_data
                 WHERE cell_value IS NOT NULL 
                   AND cell_value != 'null'"
      }
    }
    
    sink {
      Iceberg {
        catalog_name = "iceberg_catalog"
        catalog_uri = "http://iceberg-rest-catalog:8181"
        namespace = "commodity_data"
        table = "census_economic_data"
        schema_save_mode = "RECREATE_SCHEMA"
        data_save_mode = "APPEND_DATA"
      }
    }

---
# Python-based Data Ingestion Scripts (run via DolphinScheduler)
apiVersion: v1
kind: ConfigMap
metadata:
  name: commodity-ingestion-scripts
  namespace: data-platform
  labels:
    app: commodity-ingestion
data:
  ingest_alphavantage.py: |
    #!/usr/bin/env python3
    """Ingest commodity price data from AlphaVantage"""
    import requests
    import os
    from datetime import datetime, timedelta
    import pandas as pd
    from pyiceberg.catalog import load_catalog
    
    API_KEY = os.getenv('ALPHAVANTAGE_API_KEY')
    SYMBOLS = {
        'CL=F': 'crude_oil_futures',
        'NG=F': 'natural_gas_futures',
        'HO=F': 'heating_oil_futures',
        'RB=F': 'gasoline_futures'
    }
    
    def fetch_commodity_data(symbol, function='TIME_SERIES_DAILY'):
        """Fetch data from AlphaVantage"""
        url = 'https://www.alphavantage.co/query'
        params = {
            'function': function,
            'symbol': symbol,
            'apikey': API_KEY,
            'outputsize': 'compact',  # Last 100 data points
            'datatype': 'json'
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    
    def process_and_store(data, commodity_name):
        """Process and store in Iceberg"""
        time_series = data.get('Time Series (Daily)', {})
        
        records = []
        for date_str, values in time_series.items():
            records.append({
                'trade_date': date_str,
                'commodity': commodity_name,
                'open': float(values['1. open']),
                'high': float(values['2. high']),
                'low': float(values['3. low']),
                'close': float(values['4. close']),
                'volume': int(values['5. volume']),
                'source': 'ALPHAVANTAGE',
                'ingestion_time': datetime.now().isoformat()
            })
        
        df = pd.DataFrame(records)
        print(f"Fetched {len(df)} records for {commodity_name}")
        
        # Store in Iceberg
        catalog = load_catalog('iceberg_catalog', uri='http://iceberg-rest-catalog:8181')
        table = catalog.load_table('commodity_data.market_prices_intraday')
        table.append(df)
        
        return len(df)
    
    if __name__ == '__main__':
        total_records = 0
        for symbol, commodity in SYMBOLS.items():
            try:
                print(f"Fetching {commodity} data from AlphaVantage...")
                data = fetch_commodity_data(symbol)
                count = process_and_store(data, commodity)
                total_records += count
                print(f"✅ {commodity}: {count} records ingested")
            except Exception as e:
                print(f"❌ Error fetching {commodity}: {e}")
        
        print(f"\n Total: {total_records} records ingested from AlphaVantage")

  ingest_polygon.py: |
    #!/usr/bin/env python3
    """Ingest commodity data from Polygon.io"""
    import requests
    import os
    from datetime import datetime, timedelta
    import pandas as pd
    from pyiceberg.catalog import load_catalog
    
    API_KEY = os.getenv('POLYGON_API_KEY')
    
    # Commodity tickers on Polygon
    TICKERS = {
        'C:CL': 'crude_oil_wti',
        'C:NG': 'natural_gas',
        'C:HO': 'heating_oil',
        'C:RB': 'gasoline',
        'C:LNG': 'lng'
    }
    
    def fetch_polygon_data(ticker, days_back=30):
        """Fetch aggregated bars from Polygon.io"""
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days_back)
        
        url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}"
        headers = {'Authorization': f'Bearer {API_KEY}'}
        
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    
    def process_polygon_data(data, commodity_name):
        """Process Polygon data and store in Iceberg"""
        results = data.get('results', [])
        
        records = []
        for bar in results:
            records.append({
                'trade_date': datetime.fromtimestamp(bar['t']/1000).date().isoformat(),
                'commodity': commodity_name,
                'open': bar['o'],
                'high': bar['h'],
                'low': bar['l'],
                'close': bar['c'],
                'volume': bar['v'],
                'vwap': bar.get('vw', None),
                'transactions': bar.get('n', None),
                'source': 'POLYGON',
                'ingestion_time': datetime.now().isoformat()
            })
        
        df = pd.DataFrame(records)
        print(f"Fetched {len(df)} records for {commodity_name}")
        
        # Store in Iceberg
        catalog = load_catalog('iceberg_catalog', uri='http://iceberg-rest-catalog:8181')
        table = catalog.load_table('commodity_data.polygon_market_data')
        table.append(df)
        
        return len(df)
    
    if __name__ == '__main__':
        total_records = 0
        for ticker, commodity in TICKERS.items():
            try:
                print(f"Fetching {commodity} from Polygon.io...")
                data = fetch_polygon_data(ticker)
                count = process_polygon_data(data, commodity)
                total_records += count
                print(f"✅ {commodity}: {count} records")
            except Exception as e:
                print(f"❌ Error for {commodity}: {e}")
        
        print(f"\nTotal: {total_records} records from Polygon.io")

  ingest_gie_storage.py: |
    #!/usr/bin/env python3
    """Ingest European gas storage data from GIE (AGSI)"""
    import requests
    import os
    from datetime import datetime, timedelta
    import pandas as pd
    from pyiceberg.catalog import load_catalog
    
    API_KEY = os.getenv('GIE_API_KEY')
    
    def fetch_gie_storage(days_back=90):
        """Fetch gas storage data from GIE AGSI API"""
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=days_back)
        
        url = 'https://agsi.gie.eu/api'
        headers = {'x-key': API_KEY}
        params = {
            'country': 'EU',
            'from': start_date.isoformat(),
            'to': end_date.isoformat(),
            'size': 300
        }
        
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        
        return data.get('data', [])
    
    def process_gie_data(records):
        """Process and store GIE storage data"""
        processed = []
        for record in records:
            processed.append({
                'gas_day': record.get('gasDayStart'),
                'country': record.get('name'),
                'country_code': record.get('code'),
                'gas_in_storage_gwh': float(record.get('gasInStorage', 0)),
                'consumption_gwh': float(record.get('consumption', 0)),
                'storage_full_pct': float(record.get('full', 0)),
                'injection_gwh': float(record.get('injection', 0)),
                'withdrawal_gwh': float(record.get('withdrawal', 0)),
                'working_gas_volume_gwh': float(record.get('workingGasVolume', 0)),
                'trend': float(record.get('trend', 0)),
                'source': 'GIE_AGSI',
                'ingestion_time': datetime.now().isoformat()
            })
        
        df = pd.DataFrame(processed)
        print(f"Processed {len(df)} GIE storage records")
        
        # Store in Iceberg
        catalog = load_catalog('iceberg_catalog', uri='http://iceberg-rest-catalog:8181')
        table = catalog.load_table('commodity_data.gas_storage_europe')
        table.append(df)
        
        return len(df)
    
    if __name__ == '__main__':
        try:
            print("Fetching European gas storage data from GIE...")
            records = fetch_gie_storage(days_back=90)
            count = process_gie_data(records)
            print(f"✅ Successfully ingested {count} GIE storage records")
        except Exception as e:
            print(f"❌ Error ingesting GIE data: {e}")
            raise

  ingest_census_economic.py: |
    #!/usr/bin/env python3
    """Ingest US Census economic indicators"""
    import requests
    import os
    from datetime import datetime
    import pandas as pd
    from pyiceberg.catalog import load_catalog
    
    API_KEY = os.getenv('CENSUS_API_KEY')
    
    def fetch_census_data(dataset='timeseries/eits/resconst', year_from=2020):
        """Fetch data from US Census Bureau"""
        url = f'https://api.census.gov/data/{dataset}'
        params = {
            'get': 'cell_value,data_type_code,time_slot_id,category_code,error_data',
            'key': API_KEY,
            'time': f'from {year_from}'
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        return response.json()
    
    def process_census_data(data):
        """Process Census data into structured format"""
        # First row is headers
        headers = data[0]
        records = data[1:]
        
        processed = []
        for record in records:
            row_dict = dict(zip(headers, record))
            try:
                processed.append({
                    'time_period': row_dict.get('time_slot_id'),
                    'category_code': row_dict.get('category_code'),
                    'data_type': row_dict.get('data_type_code'),
                    'value': float(row_dict.get('cell_value', 0)) if row_dict.get('cell_value') and row_dict.get('cell_value') != 'null' else None,
                    'error_data': row_dict.get('error_data'),
                    'source': 'US_CENSUS',
                    'dataset': 'resconst',
                    'ingestion_time': datetime.now().isoformat()
                })
            except (ValueError, TypeError):
                continue
        
        df = pd.DataFrame(processed)
        df = df[df['value'].notnull()]  # Remove null values
        
        print(f"Processed {len(df)} US Census records")
        
        # Store in Iceberg
        catalog = load_catalog('iceberg_catalog', uri='http://iceberg-rest-catalog:8181')
        table = catalog.load_table('commodity_data.census_economic_data')
        table.append(df)
        
        return len(df)
    
    if __name__ == '__main__':
        try:
            print("Fetching US Census economic indicators...")
            data = fetch_census_data()
            count = process_census_data(data)
            print(f"✅ Successfully ingested {count} Census records")
        except Exception as e:
            print(f"❌ Error ingesting Census data: {e}")
            raise

