# SeaTunnel Job: Data Quality Monitoring
# Validates data quality across Iceberg tables and reports issues

env {
  execution.parallelism = 1
  job.mode = "BATCH"
  checkpoint.interval = 300000
}

source {
  Iceberg {
    catalog_name = "rest"
    catalog_type = "rest"
    warehouse = "s3://iceberg-warehouse/"
    uri = "http://iceberg-rest-catalog:8181"

    # Read from multiple tables for quality checks
    sql = "
      SELECT
        'customers_cdc' as table_name,
        COUNT(*) as record_count,
        COUNT(DISTINCT id) as unique_ids,
        COUNT(CASE WHEN name IS NULL THEN 1 END) as null_names,
        COUNT(CASE WHEN email IS NULL THEN 1 END) as null_emails,
        MIN(created_at) as min_created_at,
        MAX(created_at) as max_created_at
      FROM ods.customers_cdc
      UNION ALL
      SELECT
        'events' as table_name,
        COUNT(*) as record_count,
        COUNT(DISTINCT event_id) as unique_ids,
        COUNT(CASE WHEN event_type IS NULL THEN 1 END) as null_event_types,
        COUNT(CASE WHEN event_timestamp IS NULL THEN 1 END) as null_timestamps,
        MIN(event_timestamp) as min_created_at,
        MAX(event_timestamp) as max_created_at
      FROM raw.events
    "
    result_table_name = "quality_metrics"

    # S3/MinIO configuration for reading
    s3.access-key-id = "minioadmin"
    s3.secret-access-key = "minioadmin123"
    s3.region = "us-east-1"
    s3.endpoint = "http://minio-service:9000"
  }
}

transform {
  # Calculate quality scores
  Calculate {
    source_table_name = "quality_metrics"
    result_table_name = "quality_scores"

    # Quality score calculation (0-100)
    # Based on null percentages and uniqueness
    expression = "
      CASE
        WHEN record_count = 0 THEN 0
        ELSE
          100 - (
            (null_names / record_count * 20) +
            (null_emails / record_count * 20) +
            (null_event_types / record_count * 20) +
            (null_timestamps / record_count * 20) +
            (CASE WHEN unique_ids < record_count * 0.9 THEN 20 ELSE 0 END)
          )
      END as quality_score
    "
  }

  # Filter for poor quality data
  Filter {
    source_table_name = "quality_scores"
    result_table_name = "quality_issues"
    condition = "quality_score < 80"
  }
}

sink {
  # Send quality issues to Kafka for alerting
  Kafka {
    bootstrap.servers = "kafka-service:9092"
    topic = "data-quality-alerts"
    format = "json"

    kafka_config = {
      acks = "all"
      retries = 3
      max.in.flight.requests.per.connection = 1
    }
  }

  # Also write to Iceberg for historical analysis
  Iceberg {
    catalog_name = "rest"
    catalog_type = "rest"
    warehouse = "s3://iceberg-warehouse/"
    uri = "http://iceberg-rest-catalog:8181"
    database = "monitoring"
    table = "data_quality_metrics"

    # S3/MinIO configuration
    s3.access-key-id = "minioadmin"
    s3.secret-access-key = "minioadmin123"
    s3.region = "us-east-1"
    s3.endpoint = "http://minio-service:9000"

    # Partition by date
    partition_by = ["table_name", "year(CURRENT_DATE())"]

    # Schema
    schema = {
      fields {
        table_name = "string"
        record_count = "bigint"
        unique_ids = "bigint"
        null_names = "bigint"
        null_emails = "bigint"
        null_event_types = "bigint"
        null_timestamps = "bigint"
        quality_score = "double"
        check_timestamp = "timestamp"
      }
    }
  }
}
