# Comprehensive AlertManager rules for 254Carbon Data Platform
apiVersion: v1
kind: ConfigMap
metadata:
  name: platform-alert-rules
  namespace: monitoring
  labels:
    prometheus: kube-prometheus-stack-prometheus
    role: alert-rules
data:
  platform-alerts.yaml: |
    groups:
    - name: data-platform-alerts
      interval: 30s
      rules:
      # Pod Health Alerts
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{namespace="data-platform"}[15m]) > 0
        for: 5m
        labels:
          severity: warning
          namespace: data-platform
        annotations:
          summary: "Pod {{`{{ $labels.pod }}`}} is crash looping"
          description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} is restarting frequently ({{`{{ $value }}`}} restarts in 15m)"
      
      - alert: PodNotReady
        expr: kube_pod_status_phase{namespace="data-platform",phase!~"Running|Succeeded"} == 1
        for: 10m
        labels:
          severity: warning
          namespace: data-platform
        annotations:
          summary: "Pod {{`{{ $labels.pod }}`}} is not ready"
          description: "Pod {{`{{ $labels.pod }}`}} in namespace {{`{{ $labels.namespace }}`}} has been in {{`{{ $labels.phase }}`}} state for more than 10 minutes"
      
      # Resource Utilization Alerts
      - alert: HighMemoryUsage
        expr: (container_memory_working_set_bytes{namespace="data-platform"} / container_spec_memory_limit_bytes{namespace="data-platform"}) > 0.9
        for: 5m
        labels:
          severity: warning
          namespace: data-platform
        annotations:
          summary: "High memory usage for {{`{{ $labels.pod }}`}}"
          description: "Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.pod }}`}} is using {{`{{ $value | humanizePercentage }}`}} of its memory limit"
      
      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{namespace="data-platform"}[5m]) > 0.9
        for: 10m
        labels:
          severity: warning
          namespace: data-platform
        annotations:
          summary: "High CPU usage for {{`{{ $labels.pod }}`}}"
          description: "Container {{`{{ $labels.container }}`}} in pod {{`{{ $labels.pod }}`}} is using high CPU ({{`{{ $value | humanizePercentage }}`}})"
      
      # Storage Alerts
      - alert: PersistentVolumeFillingUp
        expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} is filling up"
          description: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} in namespace {{`{{ $labels.namespace }}`}} is {{`{{ $value | humanizePercentage }}`}} full"
      
      - alert: PersistentVolumeCritical
        expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} is critically full"
          description: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} in namespace {{`{{ $labels.namespace }}`}} is {{`{{ $value | humanizePercentage }}`}} full - immediate action required"
      
      # Service-Specific Alerts
      - alert: PostgreSQLDown
        expr: up{job="postgres-shared"} == 0
        for: 2m
        labels:
          severity: critical
          service: postgresql
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 2 minutes"
      
      - alert: MinIODown
        expr: up{job="minio"} == 0
        for: 2m
        labels:
          severity: critical
          service: minio
        annotations:
          summary: "MinIO is down"
          description: "MinIO object storage has been down for more than 2 minutes"
      
      - alert: DataHubGMSDown
        expr: up{job="datahub-gms"} == 0
        for: 5m
        labels:
          severity: warning
          service: datahub
        annotations:
          summary: "DataHub GMS is down"
          description: "DataHub Graph Metadata Service has been down for more than 5 minutes"
      
      - alert: KafkaDown
        expr: up{job="kafka"} == 0
        for: 2m
        labels:
          severity: critical
          service: kafka
        annotations:
          summary: "Kafka is down"
          description: "Kafka message broker has been down for more than 2 minutes"
      
    - name: cloudflare-tunnel-alerts
      interval: 30s
      rules:
      - alert: CloudflareTunnelDown
        expr: up{job="cloudflare-tunnel"} == 0
        for: 2m
        labels:
          severity: critical
          component: networking
        annotations:
          summary: "Cloudflare Tunnel is down"
          description: "Cloudflare Tunnel has been down for more than 2 minutes - all external access is affected"
      
      - alert: CloudflareTunnelLowConnections
        expr: cloudflared_tunnel_connections_registered < 2
        for: 5m
        labels:
          severity: warning
          component: networking
        annotations:
          summary: "Cloudflare Tunnel has low connections"
          description: "Cloudflare Tunnel has {{`{{ $value }}`}} connections (expected 4+)"
      
      - alert: CloudflareTunnelPodNotReady
        expr: kube_deployment_status_replicas_available{deployment="cloudflared",namespace="cloudflare-tunnel"} < 2
        for: 2m
        labels:
          severity: critical
          component: networking
        annotations:
          summary: "Cloudflare Tunnel pods not ready"
          description: "Only {{`{{ $value }}`}} Cloudflare Tunnel pods are available (expected 2)"
    
    - name: ingress-alerts
      interval: 30s
      rules:
      - alert: IngressControllerDown
        expr: up{job="ingress-nginx-controller"} == 0
        for: 2m
        labels:
          severity: critical
          component: networking
        annotations:
          summary: "NGINX Ingress Controller is down"
          description: "NGINX Ingress Controller has been down for more than 2 minutes"
      
      - alert: HighIngressErrorRate
        expr: rate(nginx_ingress_controller_requests{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          component: networking
        annotations:
          summary: "High error rate on ingress"
          description: "Ingress {{`{{ $labels.ingress }}`}} has high 5xx error rate: {{`{{ $value | humanizePercentage }}`}}"
      
      - alert: IngressCertificateExpiringSoon
        expr: (nginx_ingress_controller_ssl_expire_time_seconds - time()) / 86400 < 7
        for: 1h
        labels:
          severity: warning
          component: security
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate for {{`{{ $labels.host }}`}} expires in {{`{{ $value }}`}} days"
    
    - name: node-alerts
      interval: 30s
      rules:
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Node {{`{{ $labels.node }}`}} is not ready"
          description: "Node {{`{{ $labels.node }}`}} has been not ready for more than 5 minutes"
      
      - alert: NodeHighMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Node {{`{{ $labels.node }}`}} under memory pressure"
          description: "Node {{`{{ $labels.node }}`}} is experiencing memory pressure"
      
      - alert: NodeHighDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Node {{`{{ $labels.node }}`}} under disk pressure"
          description: "Node {{`{{ $labels.node }}`}} is experiencing disk pressure"
    
    - name: backup-alerts
      interval: 60s
      rules:
      - alert: VeleroBackupFailed
        expr: velero_backup_failure_total > 0
        for: 1m
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "Velero backup failed"
          description: "Velero backup {{`{{ $labels.schedule }}`}} has failed"
      
      - alert: VeleroBackupTooOld
        expr: time() - velero_backup_last_successful_timestamp > 172800
        for: 1h
        labels:
          severity: warning
          component: backup
        annotations:
          summary: "No recent Velero backups"
          description: "No successful Velero backup for {{`{{ $labels.schedule }}`}} in the last 48 hours"


