# PrometheusRule resources for critical alerting
# Defines alert conditions and notifications

---
# Pod Health & Restarts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: pod-health-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: pod-health
    interval: 30s
    rules:
    - alert: PodRestartingTooOften
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0.5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} restarting too often"
        description: "Pod has restarted {{`{{ $value | humanize }}`}} times in the last 15 minutes"
    
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[1h]) > 5
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} crash looping"
        description: "Pod has crashed {{`{{ $value | humanize }}`}} times in the last hour"
    
    - alert: PodNotHealthy
      expr: kube_pod_status_phase{phase=~"Failed|Unknown|Pending"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} not healthy"
        description: "Pod has been in {{`{{ $labels.phase }}`}} state for more than 10 minutes"

---
# Storage & Disk Space
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: storage
    interval: 30s
    rules:
    - alert: PersistentVolumeUsageHigh
      expr: kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} usage above 85%"
        description: "PVC {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}} is {{`{{ humanizePercentage $value }}`}} full"
    
    - alert: PersistentVolumeLowSpace
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.05
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} critically low on space"
        description: "Only {{`{{ humanizePercentage $value }}`}} space remaining on {{`{{ $labels.namespace }}`}}/{{`{{ $labels.persistentvolumeclaim }}`}}"
    
    - alert: PersistentVolumeInodeLow
      expr: kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{`{{ $labels.persistentvolumeclaim }}`}} inode usage high"
        description: "Only {{`{{ humanizePercentage $value }}`}} of inodes remaining"

---
# Memory & CPU
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: resource-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: resources
    interval: 30s
    rules:
    - alert: PodMemoryUsageHigh
      expr: |
        (sum(container_memory_working_set_bytes) by (namespace, pod) / 
         sum(container_spec_memory_limit_bytes) by (namespace, pod)) > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} memory usage above 90%"
        description: "Pod memory usage is {{`{{ humanizePercentage $value }}`}}"
    
    - alert: ContainerCpuThrottled
      expr: rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container {{`{{ $labels.namespace }}`}}/{{`{{ $labels.pod }}`}} CPU throttled"
        description: "Container CPU has been throttled for {{`{{ $value | humanize }}`}}% of the time"
    
    - alert: NodeMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{`{{ $labels.node }}`}} has MemoryPressure"
        description: "Node is experiencing memory pressure"

---
# Database Health
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: database-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: databases
    interval: 30s
    rules:
    - alert: PostgreSQLDown
      expr: up{job="postgres"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "PostgreSQL {{`{{ $labels.instance }}`}} is down"
        description: "PostgreSQL database is not responding to queries"
    
    - alert: PostgreSQLTooManyConnections
      expr: sum(pg_stat_activity_count) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PostgreSQL connection pool near limit"
        description: "{{`{{ $value | humanize }}`}} of 100 connections in use"
    
    - alert: MySQLDown
      expr: up{job="mysql"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "MySQL {{`{{ $labels.instance }}`}} is down"
        description: "MySQL database is not responding to queries"

---
# Service Availability
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: service-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: services
    interval: 30s
    rules:
    - alert: ServiceDown
      expr: up{job=~"kafka|elasticsearch|zookeeper|redis"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Service {{`{{ $labels.job }}`}} is down"
        description: "{{`{{ $labels.job }}`}} at {{`{{ $labels.instance }}`}} is not responding"
    
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High error rate for {{`{{ $labels.service }}`}}"
        description: "Error rate is {{`{{ humanizePercentage $value }}`}}"
    
    - alert: HighLatency
      expr: histogram_quantile(0.99, http_request_duration_seconds_bucket) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High latency for {{`{{ $labels.service }}`}}"
        description: "99th percentile latency is {{`{{ humanizeDuration $value }}`}}"

---
# Deployment & ReplicaSet
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: deployment-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: deployments
    interval: 30s
    rules:
    - alert: DeploymentReplicasMismatch
      expr: |
        kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Deployment {{`{{ $labels.namespace }}`}}/{{`{{ $labels.deployment }}`}} replicas mismatch"
        description: "Deployment has not matched the expected number of replicas for more than 10 minutes"
    
    - alert: StatefulSetReplicasMismatch
      expr: |
        kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "StatefulSet {{`{{ $labels.namespace }}`}}/{{`{{ $labels.statefulset }}`}} replicas mismatch"
        description: "StatefulSet has not matched the expected number of ready replicas"

---
# Kubernetes Node & Cluster Health
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cluster-alerts
  namespace: monitoring
  labels:
    alertmanager: main
spec:
  groups:
  - name: cluster
    interval: 30s
    rules:
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{`{{ $labels.node }}`}} is not ready"
        description: "Node has been unready for more than 5 minutes"
    
    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{`{{ $labels.node }}`}} has DiskPressure"
        description: "Node is experiencing disk pressure"
    
    - alert: KubeletDown
      expr: up{job="kubelet"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kubelet on {{`{{ $labels.node }}`}} is down"
        description: "Kubelet is not responding to queries"
