# Spark History Server for monitoring Spark applications
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-history-server
  namespace: {{ default "data-platform" .Values.global.namespace }}
  labels:
    app: spark-history-server
    component: compute
spec:
  replicas: {{ default 1 .Values.sparkHistory.replicaCount }}
  selector:
    matchLabels:
      app: spark-history-server
  template:
    metadata:
      labels:
        app: spark-history-server
        component: compute
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      imagePullSecrets: []
      containers:
      - name: spark-history-server
        image: {{ default "apache/spark:3.5.0" .Values.sparkHistory.image }}
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        ports:
          - containerPort: 18080
            name: history-ui
          - containerPort: 18081
            name: history-admin
        command:
        - /opt/spark/bin/spark-class
        - org.apache.spark.deploy.history.HistoryServer
        env:
        - name: SPARK_HISTORY_OPTS
          value: '-Dspark.history.fs.logDirectory={{ default "s3a://spark-logs/" .Values.sparkHistory.logDirectory }} -Dspark.history.fs.cleaner.enabled=true -Dspark.history.fs.cleaner.maxAge=7d -Dspark.history.ui.port=18080 -Dspark.history.admin.port=18081 -Dspark.hadoop.fs.s3a.endpoint={{ default "http://minio-service:9000" .Values.sparkHistory.s3Endpoint }} -Dspark.hadoop.fs.s3a.path.style.access=true -Dspark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dspark.hadoop.fs.s3a.connection.ssl.enabled=false'
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: access-key
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: secret-key
        volumeMounts:
        {{- if .Values.sparkHistory.usePersistentVolume }}
        - name: spark-logs
          mountPath: /opt/spark/work
        {{- end }}
        - name: spark-conf
          mountPath: /opt/spark/conf
        {{- $historyResources := and .Values.sparkHistory (index .Values.sparkHistory "resources") }}
        {{- if $historyResources }}
        resources:
{{ toYaml $historyResources | nindent 10 }}
        {{- else }}
        resources:
          limits:
            memory: "2Gi"
            cpu: "1000m"
          requests:
            memory: "1Gi"
            cpu: "500m"
        {{- end }}
        livenessProbe:
          httpGet:
            path: /
            port: 18080
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 18080
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      {{- if .Values.sparkHistory.usePersistentVolume }}
      - name: spark-logs
        persistentVolumeClaim:
          claimName: spark-logs-pvc
      {{- end }}
      - name: spark-conf
        configMap:
          name: spark-history-conf
---
# Spark History Server Service
apiVersion: v1
kind: Service
metadata:
  name: spark-history-server
  namespace: {{ default "data-platform" .Values.global.namespace }}
  labels:
    app: spark-history-server
spec:
  ports:
  - port: 18080
    targetPort: 18080
    name: history-ui
  selector:
    app: spark-history-server
  type: ClusterIP

---
# Ingress for Spark History Server UI
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: spark-history-ingress
  namespace: {{ default "data-platform" .Values.global.namespace }}
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rewrite-target: /
    portal.254carbon.com/service-id: "spark-history"
    portal.254carbon.com/service-name: "Spark History Server"
    portal.254carbon.com/service-category: "data-platform"
    portal.254carbon.com/service-description: "Spark job history and diagnostics"
    portal.254carbon.com/service-icon: "ðŸ”¥"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - spark-history.254carbon.com
    secretName: spark-history-tls
  rules:
  - host: spark-history.254carbon.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-history-server
            port:
              number: 18080
{{- if .Values.sparkHistory.usePersistentVolume }}
---
# Persistent Volume Claim for Spark logs
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: spark-logs-pvc
  namespace: {{ default "data-platform" .Values.global.namespace }}
  labels:
    app: spark-history-server
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: {{ default "50Gi" (index .Values.sparkHistory.persistentVolume "size") }}
  storageClassName: {{ default "local-storage-standard" (index .Values.sparkHistory.persistentVolume "storageClass") }}
---
{{- end }}
# ConfigMap for Spark History Server configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-history-conf
  namespace: {{ default "data-platform" .Values.global.namespace }}
  labels:
    app: spark-history-server
data:
  spark-defaults.conf: |
    # Spark configuration for history server
    spark.history.fs.logDirectory {{ default "s3a://spark-logs/" .Values.sparkHistory.logDirectory }}
    spark.history.fs.cleaner.enabled true
    spark.history.fs.cleaner.maxAge 7d
    spark.history.ui.port 18080
    spark.history.admin.port 18081

    # Object storage configuration
    spark.hadoop.fs.s3a.endpoint {{ default "http://minio-service:9000" .Values.sparkHistory.s3Endpoint }}
    spark.hadoop.fs.s3a.path.style.access true
    spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.ssl.enabled false

    # Spark SQL and general settings
    spark.sql.adaptive.enabled true
    spark.sql.adaptive.coalescePartitions.enabled true
    spark.serializer org.apache.spark.serializer.KryoSerializer

  log4j.properties: |
    # Logging configuration
    log4j.rootCategory=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n
