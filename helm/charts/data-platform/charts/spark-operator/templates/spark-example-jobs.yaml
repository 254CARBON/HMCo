# Example 1: ETL Job - Kafka to Iceberg
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: kafka-to-iceberg-etl
  namespace: data-platform
  labels:
    app: spark
    component: etl
spec:
  type: Python
  mode: cluster
  image: apache/spark:3.5.0
  imagePullPolicy: IfNotPresent
  pythonVersion: "3"

  mainApplicationFile: s3a://spark-code/etl/kafka_to_iceberg.py

  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20

  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "2g"
    labels:
      version: 3.5.0
    serviceAccount: spark-operator-spark
    env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow-service:5000"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key
    configMaps:
      - name: spark-defaults
        path: /opt/spark/conf

  executor:
    cores: 2
    instances: 2
    memory: "2g"
    labels:
      version: 3.5.0
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key

  sparkConf:
    spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type: rest
    spark.sql.catalog.iceberg.uri: http://iceberg-rest-catalog:8181
    spark.sql.catalog.iceberg.warehouse: s3://iceberg-warehouse/
    spark.hadoop.fs.s3a.endpoint: http://minio-service:9000
    spark.hadoop.fs.s3a.access.key: minioadmin
    spark.hadoop.fs.s3a.secret.key: minioadmin123
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"

---
# Example 2: Analytics Job - SQL on Iceberg
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: iceberg-sql-analytics
  namespace: data-platform
  labels:
    app: spark
    component: analytics
spec:
  type: Python
  mode: cluster
  image: apache/spark:3.5.0
  imagePullPolicy: IfNotPresent
  pythonVersion: "3"

  mainApplicationFile: s3a://spark-code/analytics/iceberg_sql_query.py
  arguments:
    - "--catalog"
    - "iceberg"
    - "--database"
    - "raw"
    - "--table"
    - "events"
    - "--output"
    - "s3a://analytics-results/events-summary.parquet"

  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 2
    onFailureRetryInterval: 15

  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "2g"
    labels:
      version: 3.5.0
    serviceAccount: spark-operator-spark
    env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow-service:5000"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key

  executor:
    cores: 2
    instances: 3
    memory: "2g"
    labels:
      version: 3.5.0
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key

  sparkConf:
    spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type: rest
    spark.sql.catalog.iceberg.uri: http://iceberg-rest-catalog:8181
    spark.sql.catalog.iceberg.warehouse: s3://iceberg-warehouse/
    spark.hadoop.fs.s3a.endpoint: http://minio-service:9000
    spark.hadoop.fs.s3a.access.key: minioadmin
    spark.hadoop.fs.s3a.secret.key: minioadmin123
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"

---
# Example 3: Data Transformation with MLFlow Tracking
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: data-enrichment-transform
  namespace: data-platform
  labels:
    app: spark
    component: transformation
spec:
  type: Python
  mode: cluster
  image: apache/spark:3.5.0
  imagePullPolicy: IfNotPresent
  pythonVersion: "3"

  mainApplicationFile: s3a://spark-code/transform/enrichment_pipeline.py
  arguments:
    - "--source-table"
    - "ods.customers"
    - "--target-table"
    - "dw.customers_enriched"
    - "--enrichment-data"
    - "s3a://enrichment-data/demographics.csv"

  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 20
    onSubmissionFailureRetries: 5

  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "3g"
    labels:
      version: 3.5.0
    serviceAccount: spark-operator-spark
    env:
      - name: MLFLOW_TRACKING_URI
        value: "http://mlflow-service:5000"
      - name: MLFLOW_EXPERIMENT_NAME
        value: "data-enrichment"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key

  executor:
    cores: 2
    instances: 4
    memory: "2g"
    labels:
      version: 3.5.0
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key

  sparkConf:
    spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type: rest
    spark.sql.catalog.iceberg.uri: http://iceberg-rest-catalog:8181
    spark.sql.catalog.iceberg.warehouse: s3://iceberg-warehouse/
    spark.hadoop.fs.s3a.endpoint: http://minio-service:9000
    spark.hadoop.fs.s3a.access.key: minioadmin
    spark.hadoop.fs.s3a.secret.key: minioadmin123
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.connection.ssl.enabled: "false"

---
# Example 4: Deequ Quality Check SparkApplication
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: deequ-quality-check
  namespace: data-platform
  labels:
    app: spark
    component: quality
spec:
  type: Python
  mode: cluster
  image: apache/spark:3.5.0
  imagePullPolicy: IfNotPresent
  pythonVersion: "3"

  mainApplicationFile: s3a://spark-code/quality/deequ_check_runner.py
  arguments:
    - "--table"
    - "raw.customers"
    - "--catalog"
    - "iceberg"
    - "--output-table"
    - "monitoring.deequ_results"

  sparkVersion: "3.5.0"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 2
    onFailureRetryInterval: 10

  deps:
    packages:
      - org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0
      - com.amazon.deequ:deequ:2.0.3-spark-3.5

  driver:
    cores: 2
    coreLimit: "2000m"
    memory: "2g"
    labels:
      version: 3.5.0
    serviceAccount: spark-operator-spark
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: secret-key
    configMaps:
      - name: spark-deequ
        path: /opt/spark/conf/deequ

  executor:
    cores: 2
    instances: 2
    memory: "2g"
    labels:
      version: 3.5.0
    env:
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: minio-secret
            key: access-key
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: minio-secret

