# Spark Configuration ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-defaults
  namespace: data-platform
  labels:
    app: spark
    component: configuration
data:
  spark-defaults.conf: |
    # Spark SQL and DataFrame defaults
    spark.sql.adaptive.enabled true
    spark.sql.adaptive.coalescePartitions.enabled true
    spark.sql.adaptive.skewJoin.enabled true
    spark.sql.shuffle.partitions 200
    spark.serializer org.apache.spark.serializer.KryoSerializer
    spark.kryoserializer.buffer.max 512m
    
    # Iceberg Catalog Configuration
    spark.sql.catalog.iceberg org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type rest
    spark.sql.catalog.iceberg.uri http://iceberg-rest-catalog:8181
    spark.sql.catalog.iceberg.warehouse s3://iceberg-warehouse/
    spark.sql.catalog.iceberg.io-impl org.apache.iceberg.aws.s3.S3FileIO
    spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    
    # Hudi Configuration (if needed)
    spark.sql.catalog.spark_catalog org.apache.spark.sql.hudi.catalog.HoodieCatalog
    hoodie.datasource.write.recordkey.field id
    hoodie.datasource.write.partitionpath.field partition
    hoodie.datasource.write.precombine.field timestamp
    
    # S3/MinIO Configuration for Spark
    spark.hadoop.fs.s3a.endpoint http://minio-service:9000
    spark.hadoop.fs.s3a.access.key minioadmin
    spark.hadoop.fs.s3a.secret.key minioadmin123
    spark.hadoop.fs.s3a.path.style.access true
    spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
    spark.hadoop.fs.s3a.connection.ssl.enabled false
    spark.hadoop.fs.s3a.fast.upload true
    spark.hadoop.fs.s3a.fast.upload.active.blocks 32
    
    # Hadoop configuration
    spark.hadoop.hive.metastore.uris thrift://hive-metastore:9083
    spark.hadoop.fs.defaultFS s3a://spark-data/
    
    # Kafka Configuration (requires mounting kafka-platform-apps-tls at /etc/kafka/secrets)
    spark.kafka.bootstrap.servers kafka-service:9093
    spark.kafka.security.protocol SSL
    spark.kafka.ssl.keystore.location /etc/kafka/secrets/user.p12
    spark.kafka.ssl.truststore.location /etc/kafka/secrets/user.p12
    spark.kafka.ssl.keystore.password ${KAFKA_USER_PASSWORD}
    spark.kafka.ssl.truststore.password ${KAFKA_USER_PASSWORD}
    spark.kafka.ssl.key.password ${KAFKA_USER_PASSWORD}
    spark.kubernetes.driver.secrets kafka-platform-apps-tls=/etc/kafka/secrets
    spark.kubernetes.executor.secrets kafka-platform-apps-tls=/etc/kafka/secrets
    spark.kubernetes.driver.secretKeyRef.KAFKA_USER_PASSWORD kafka-platform-apps-tls:user.password
    spark.kubernetes.executor.secretKeyRef.KAFKA_USER_PASSWORD kafka-platform-apps-tls:user.password
    spark.kafka.key.deserializer org.apache.kafka.common.serialization.StringDeserializer
    spark.kafka.value.deserializer org.apache.kafka.common.serialization.StringDeserializer
    
    # MLFlow Tracking Configuration
    spark.mlflow.trackingUri http://mlflow-service:5000
    spark.mlflow.logArtifactsToS3 true
    spark.mlflow.artifactS3Endpoint http://minio-service:9000
    
    # Deequ Configuration
    spark.jars.packages com.amazon.deequ:deequ:2.0.3-spark-3.5
    
    # Memory and performance tuning
    spark.sql.broadcastTimeout 600
    spark.network.timeout 600s
    spark.executor.heartbeatInterval 60s
    
    # Logging configuration
    log4j.rootCategory=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
    
  log4j.properties: |
    # Spark Logging Configuration
    log4j.rootCategory=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
    
    # Suppress verbose logging
    log4j.logger.org.apache.spark.scheduler.cluster=INFO
    log4j.logger.org.apache.spark.executor.Executor=INFO
    log4j.logger.org.apache.parquet=WARN
    log4j.logger.org.apache.hadoop=WARN
    log4j.logger.org.apache.hadoop.fs.s3a=WARN
---
# MLFlow Integration ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-mlflow
  namespace: data-platform
  labels:
    app: spark
    component: mlflow
data:
  mlflow-config.sh: |
    #!/bin/bash
    # MLFlow configuration for Spark jobs
    export MLFLOW_TRACKING_URI=http://mlflow-service:5000
    export MLFLOW_REGISTRY_URI=http://mlflow-service:5000
    export AWS_ACCESS_KEY_ID=minioadmin
    export AWS_SECRET_ACCESS_KEY=minioadmin123
    export MLFLOW_S3_ENDPOINT_URL=http://minio-service:9000
    
    # For artifact uploads
    export AWS_S3_BUCKET_NAME=mlflow
    export MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow/artifacts
---
# Deequ Configuration ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-deequ
  namespace: data-platform
  labels:
    app: spark
    component: deequ
data:
  deequ-defaults.conf: |
    # Deequ Data Quality Framework Configuration
    deequ.checks.enabled=true
    deequ.profiling.enabled=true
    deequ.anomaly.detection.enabled=true
    
    # Quality check thresholds (0-100)
    deequ.quality.threshold.critical=80
    deequ.quality.threshold.warning=90
    
    # Profiling settings
    deequ.profiling.sample.size=100000
    deequ.profiling.cache.enabled=true
    
    # Anomaly detection (IQR method)
    deequ.anomaly.detection.method=iqr
    deequ.anomaly.detection.upper.threshold=1.5
    deequ.anomaly.detection.lower.threshold=1.5
---
# Spark Driver/Executor Environment ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-env
  namespace: data-platform
  labels:
    app: spark
    component: environment
data:
  spark-env.sh: |
    #!/bin/bash
    # Spark environment setup
    
    # Java options
    export JAVA_HOME=/usr/local/openjdk-11
    export SPARK_DIST_CLASSPATH=$SPARK_DIST_CLASSPATH:/opt/hadoop/share/hadoop/*:/opt/hadoop/share/hadoop/lib/*
    
    # Iceberg libraries
    export SPARK_EXTRA_CLASSPATH=/opt/spark/jars/iceberg-*.jar:/opt/spark/jars/iceberg-spark-runtime-*.jar
    
    # S3/Hadoop configuration
    export HADOOP_S3_ENDPOINT=http://minio-service:9000
    export AWS_ACCESS_KEY_ID=minioadmin
    export AWS_SECRET_ACCESS_KEY=minioadmin123
    
    # MLFlow configuration
    export MLFLOW_TRACKING_URI=http://mlflow-service:5000
    export MLFLOW_REGISTRY_URI=http://mlflow-service:5000
    
    # Memory settings
    export SPARK_DRIVER_MEMORY=2g
    export SPARK_EXECUTOR_MEMORY=2g
    export SPARK_EXECUTOR_CORES=2
    
    # Logging
    export SPARK_LOG_DIR=/var/log/spark
    mkdir -p $SPARK_LOG_DIR
