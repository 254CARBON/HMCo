# DolphinScheduler Workflow Template: Spark ETL Job
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-etl-workflow-template
  namespace: data-platform
  labels:
    app: dolphinscheduler
    component: spark-workflows
data:
  spark-etl-workflow.json: |
    {
      "name": "Spark ETL Pipeline Template",
      "description": "Template for submitting Spark ETL jobs via DolphinScheduler",
      "tenantCode": "admin",
      "schedule": "0 3 * * *",
      "timeZone": "UTC",
      "tasks": [
        {
          "name": "spark_etl_job",
          "type": "SPARK",
          "params": {
            "mainClass": "org.apache.spark.sql.SparkSession",
            "mainJarFile": "s3a://spark-code/etl/spark-etl-assembly.jar",
            "sparkSubmitParams": "--master k8s://https://kubernetes.default --deploy-mode cluster --class org.apache.spark.sql.SparkSession",
            "driverMemory": "2g",
            "driverCores": 2,
            "executorMemory": "2g",
            "executorCores": 2,
            "numExecutors": 2,
            "appName": "spark-etl-${DATE}"
          },
          "jobManagerMemory": "1g",
          "jobManagerCores": 1,
          "taskManagerMemory": "2g",
          "taskManagerCores": 2,
          "parallelism": 2,
          "runFlag": "NORMAL"
        }
      ],
      "postTasks": [
        {
          "name": "post_etl_check",
          "type": "SHELL",
          "params": {
            "rawScript": "echo 'ETL Job Completed Successfully' && exit 0"
          }
        }
      ]
    }
---
# DolphinScheduler Workflow Template: Quality Check Job
apiVersion: v1
kind: ConfigMap
metadata:
  name: deequ-quality-workflow-template
  namespace: data-platform
  labels:
    app: dolphinscheduler
    component: spark-workflows
data:
  deequ-quality-workflow.json: |
    {
      "name": "Deequ Quality Check Template",
      "description": "Template for submitting Deequ quality checks via DolphinScheduler",
      "tenantCode": "admin",
      "schedule": "0 2 * * *",
      "timeZone": "UTC",
      "tasks": [
        {
          "name": "deequ_quality_check",
          "type": "PYTHON",
          "params": {
            "rawScript": |
              import requests
              import json
              from datetime import datetime

              # Submit quality check SparkApplication
              spark_app = {
                  'apiVersion': 'sparkoperator.k8s.io/v1beta2',
                  'kind': 'SparkApplication',
                  'metadata': {
                      'name': f'deequ-quality-check-{datetime.now().strftime("%Y%m%d-%H%M%S")}',
                      'namespace': 'data-platform'
                  },
                  'spec': {
                      'type': 'Python',
                      'mode': 'cluster',
                      'image': 'apache/spark:3.5.0',
                      'mainApplicationFile': 's3a://spark-code/quality/deequ_quality_check.py',
                      'arguments': [
                          '--constraint-file', 's3a://deequ-config/constraints.json',
                          '--output-database', 'monitoring',
                          '--output-table', 'deequ_quality_checks'
                      ],
                      'driver': {
                          'cores': 2,
                          'memory': '2g'
                      },
                      'executor': {
                          'cores': 2,
                          'instances': 2,
                          'memory': '2g'
                      }
                  }
              }

              # Apply to Kubernetes
              import subprocess
              result = subprocess.run(
                  ['kubectl', 'apply', '-f', '-'],
                  input=json.dumps(spark_app).encode(),
                  capture_output=True
              )

              if result.returncode == 0:
                  print('Quality check SparkApplication submitted successfully')
                  exit(0)
              else:
                  print(f'Error submitting quality check: {result.stderr.decode()}')
                  exit(1)
          }
        },
        {
          "name": "wait_for_completion",
          "type": "SHELL",
          "params": {
            "rawScript": |
              job_name=$(kubectl get sparkapplications -n data-platform -o jsonpath='{.items[-1].metadata.name}')
              kubectl wait --for=condition=Succeeded sparkapplication/$job_name -n data-platform --timeout=3600s
          }
        }
      ],
      "postTasks": [
        {
          "name": "quality_check_summary",
          "type": "SHELL",
          "params": {
            "rawScript": |
              # Query quality check results
              kubectl exec -n data-platform $(kubectl get pods -n data-platform -l app=spark-history-server -o jsonpath='{.items[0].metadata.name}') -- \
                trino --server https://trino:8443 --user admin --execute 'SELECT table_name, COUNT(*) as check_count, COUNTIF(status=\"PASSED\") as passed FROM monitoring.deequ_quality_checks WHERE check_date = CURRENT_DATE GROUP BY table_name'
          }
        }
      ]
    }
---
# DolphinScheduler Workflow Template: Analytics Job
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-analytics-workflow-template
  namespace: data-platform
  labels:
    app: dolphinscheduler
    component: spark-workflows
data:
  spark-analytics-workflow.json: |
    {
      "name": "Spark Analytics Pipeline Template",
      "description": "Template for running Spark SQL analytics jobs",
      "tenantCode": "admin",
      "schedule": "0 4 * * *",
      "timeZone": "UTC",
      "globalParams": [
        {
          "name": "catalog",
          "value": "iceberg"
        },
        {
          "name": "database",
          "value": "analytics"
        },
        {
          "name": "output_format",
          "value": "parquet"
        }
      ],
      "tasks": [
        {
          "name": "spark_analytics_job",
          "type": "SPARK",
          "params": {
            "mainClass": "org.apache.spark.sql.SparkSession",
            "mainJarFile": "s3a://spark-code/analytics/spark-analytics-assembly.jar",
            "sparkSubmitParams": "--master k8s://https://kubernetes.default --deploy-mode cluster",
            "driverMemory": "3g",
            "driverCores": 2,
            "executorMemory": "2g",
            "executorCores": 2,
            "numExecutors": 3,
            "appName": "spark-analytics-${DATE}"
          }
        },
        {
          "name": "export_results",
          "type": "SHELL",
          "params": {
            "rawScript": |
              # Export analytics results to multiple formats
              kubectl exec -n data-platform $(kubectl get pods -n data-platform -l app=trino-coordinator -o jsonpath='{.items[0].metadata.name}') -- \
                trino --server https://trino:8443 --user admin --execute 'EXPORT TABLE analytics.summary TO PARQUET s3a://analytics-export/summary-${DATE}/'
          }
        }
      ]
    }
---
# DolphinScheduler Workflow Template: Data Profiling
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-profiling-workflow-template
  namespace: data-platform
  labels:
    app: dolphinscheduler
    component: spark-workflows
data:
  spark-profiling-workflow.json: |
    {
      "name": "Deequ Data Profiling Template",
      "description": "Template for running statistical profiling on tables",
      "tenantCode": "admin",
      "schedule": "0 1 * * 0",
      "timeZone": "UTC",
      "tasks": [
        {
          "name": "data_profiling_job",
          "type": "SPARK",
          "params": {
            "mainClass": "com.amazon.deequ.profiles.ColumnProfilerRunner",
            "mainJarFile": "s3a://deequ-jars/deequ-2.0.3-spark-3.5.jar",
            "sparkSubmitParams": "--master k8s://https://kubernetes.default --deploy-mode cluster --packages com.amazon.deequ:deequ:2.0.3-spark-3.5",
            "driverMemory": "3g",
            "driverCores": 2,
            "executorMemory": "2g",
            "executorCores": 2,
            "numExecutors": 3,
            "appName": "deequ-profiling-${DATE}"
          }
        },
        {
          "name": "publish_profile_results",
          "type": "SHELL",
          "params": {
            "rawScript": |
              # Push profiling results to Kafka for real-time monitoring
              kubectl exec -n data-platform $(kubectl get pods -n data-platform -l app=kafka -o jsonpath='{.items[0].metadata.name}') -- \
                /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic data-profiling-results < /tmp/profile_results.json
          }
        }
      ]
    }
