#!/usr/bin/env python3
"""
Sample Flink job implementation for executing UIS-generated streaming configurations.

This is a reference implementation showing how to execute the Flink configurations
generated by the UIS compiler using PyFlink.
"""

import json
import sys
import argparse
from typing import Dict, Any, Optional, List
import logging

from pyflink.datastream import StreamExecutionEnvironment, DataStream
from pyflink.table import EnvironmentSettings, TableEnvironment, StreamTableEnvironment
from pyflink.common import Configuration

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FlinkStreamingJob:
    """Executes UIS-generated Flink streaming configurations."""

    def __init__(self, job_config: Dict[str, Any]):
        """Initialize with job configuration."""
        self.config = job_config
        self.env = None
        self.table_env = None
        self._sink_tables_created = set()

    def create_execution_environment(self) -> StreamExecutionEnvironment:
        """Create Flink streaming execution environment."""
        # Create configuration from job config
        config = Configuration()

        flink_config = self.config["flink_config"]
        for key, value in flink_config.items():
            if key.startswith("state.") or key.startswith("execution."):
                # Convert Flink config keys to PyFlink format
                pyflink_key = key.replace(".", "_")
                config.set_string(pyflink_key, str(value))

        # Create execution environment
        self.env = StreamExecutionEnvironment.get_execution_environment(config)

        # Configure parallelism
        if "parallelism.default" in flink_config:
            self.env.set_parallelism(int(flink_config["parallelism.default"]))

        # Configure checkpointing
        if "execution.checkpointing.interval" in flink_config:
            checkpoint_interval = int(flink_config["execution.checkpointing.interval"])
            self.env.enable_checkpointing(checkpoint_interval)

        if "execution.checkpointing.mode" in flink_config:
            # Configure exactly-once processing
            self.env.get_checkpoint_config().set_checkpointing_mode(
                "EXACTLY_ONCE" if "EXACTLY_ONCE" in flink_config["execution.checkpointing.mode"] else "AT_LEAST_ONCE"
            )

        # Set job name
        job_name = self.config.get("job_name", "uis-flink-streaming")
        self.env.set_job_name(job_name)

        logger.info(f"Created Flink streaming environment: {job_name}")
        return self.env

    def create_table_environment(self) -> StreamTableEnvironment:
        """Create Flink table environment for SQL operations."""
        settings = EnvironmentSettings.new_instance() \
            .in_streaming_mode() \
            .with_configuration(self._get_table_config()) \
            .build()

        self.table_env = StreamTableEnvironment.create(self.env, settings)
        logger.info("Created Flink table environment")
        return self.table_env

    def validate_config(self) -> list:
        """Validate job configuration."""
        errors = []

        if not self.config.get("sources"):
            errors.append("No sources configured")

        if not self.config.get("sinks"):
            errors.append("No sinks configured")

        required_keys = ["job_type", "flink_config"]
        for key in required_keys:
            if key not in self.config:
                errors.append(f"Missing required configuration: {key}")

        # Validate Flink configuration
        flink_config = self.config.get("flink_config", {})
        required_flink_keys = ["state.backend", "state.checkpoints.dir"]
        for key in required_flink_keys:
            if key not in flink_config:
                errors.append(f"Missing required Flink configuration: {key}")

        return errors

    def _get_table_config(self) -> Configuration:
        """Get table configuration from job config."""
        config = Configuration()

        flink_config = self.config["flink_config"]
        for key, value in flink_config.items():
            if key.startswith("table."):
                config.set_string(key, str(value))

        return config

    def execute(self) -> bool:
        """Execute the complete streaming pipeline."""
        try:
            logger.info("Starting Flink streaming job execution")

            # Create execution environment
            env = self.create_execution_environment()

            # Create table environment for SQL operations
            table_env = self.create_table_environment()

            # Create source streams
            source_streams = self.create_source_streams()

            pipelines = self.config.get("pipelines") or []

            if pipelines:
                logger.info(f"Executing {len(pipelines)} configured pipeline(s)")
                for pipeline in pipelines:
                    source_name = pipeline.get("source", {}).get("name")
                    sink_name = pipeline.get("sink", {}).get("name")

                    if not source_name or source_name not in source_streams:
                        logger.warning(
                            f"Pipeline source '{source_name}' not found; skipping pipeline {pipeline.get('id')}"
                        )
                        continue

                    stream = source_streams[source_name]
                    transforms = self._get_transforms_for_pipeline(pipeline)
                    transformed_stream = self.apply_transforms(stream, transforms)

                    sink_config = self._get_sink_by_name(sink_name)
                    if not sink_config:
                        logger.warning(
                            f"Pipeline sink '{sink_name}' not found; skipping pipeline {pipeline.get('id')}"
                        )
                        continue

                    self.route_stream_to_sink(transformed_stream, sink_config, table_env, pipeline)
            else:
                logger.info("No pipelines defined; falling back to default sourceâ†’sink execution")

                if not source_streams:
                    raise RuntimeError("No source streams available for execution")

                default_stream = next(iter(source_streams.values()))
                transformed_stream = self.apply_transforms(default_stream, self.config.get("transforms", []))

                sinks = self.config.get("sinks")
                if not sinks:
                    raise RuntimeError("No sinks configured for execution")

                self.route_stream_to_sink(transformed_stream, sinks[0], table_env, None)

            # Execute the job
            logger.info("Executing Flink streaming job...")
            env.execute()

            return True

        except Exception as e:
            logger.error(f"Flink streaming job failed: {e}")
            raise

    def create_source_streams(self) -> Dict[str, DataStream]:
        """Create data streams from configured sources."""
        sources = self.config["sources"]
        streams: Dict[str, DataStream] = {}

        logger.info(f"Creating {len(sources)} source streams")

        for source in sources:
            source_type = source["type"]

            if source_type == "websocket":
                stream = self.create_websocket_stream(source)
            elif source_type == "kafka":
                stream = self.create_kafka_stream(source)
            elif source_type == "webhook":
                stream = self.create_webhook_stream(source)
            elif source_type == "cdc":
                stream = self.create_cdc_stream(source)
            else:
                logger.warning(f"Unsupported source type: {source_type}")
                continue

            streams[source["name"]] = stream

        return streams

    def create_websocket_stream(self, source: Dict[str, Any]) -> DataStream:
        """Create WebSocket data stream."""
        logger.info(f"Creating WebSocket stream: {source['name']}")

        # In a full implementation, this would connect to WebSocket
        # For now, create a simple data stream
        from pyflink.common import Types
        from pyflink.datastream.functions import MapFunction

        class WebSocketMapper(MapFunction):
            def map(self, value):
                # Simulate WebSocket data processing
                return {
                    "event_time": value.get("timestamp", ""),
                    "data": value.get("data", ""),
                    "metadata": {"source": source["name"]}
                }

        # Create a simple source (in reality, this would be a WebSocket connector)
        env = self.env
        stream = env.from_collection([
            {"timestamp": "2023-01-01T00:00:00Z", "data": "sample1"},
            {"timestamp": "2023-01-01T00:01:00Z", "data": "sample2"}
        ], type_info=Types.MAP(Types.STRING(), Types.STRING()))

        stream = stream.map(WebSocketMapper())
        return stream

    def create_kafka_stream(self, source: Dict[str, Any]) -> DataStream:
        """Create Kafka data stream."""
        logger.info(f"Creating Kafka stream: {source['name']}")

        # Kafka streaming source using Flink SQL
        table_env = self.table_env

        # Create Kafka source table
        table_env.execute_sql(f"""
        CREATE TABLE {source['name']} (
            event_time TIMESTAMP(3),
            data STRING,
            metadata ROW<timestamp TIMESTAMP(3), source STRING>
        ) WITH (
            'connector' = 'kafka',
            'topic' = '{",".join(source['topics'])}',
            'properties.bootstrap.servers' = '{source['bootstrap_servers']}',
            'properties.group.id' = '{source['group_id']}',
            'format' = 'json',
            'scan.startup.mode' = '{source['starting_offsets']}'
        )
        """)

        # Convert table to DataStream
        table = table_env.from_path(source['name'])
        stream = table_env.to_data_stream(table)

        return stream

    def create_webhook_stream(self, source: Dict[str, Any]) -> DataStream:
        """Create webhook data stream."""
        logger.info(f"Creating webhook stream: {source['name']}")

        # In a full implementation, this would be an HTTP server connector
        # For now, create a simple data stream
        from pyflink.common import Types
        from pyflink.datastream.functions import MapFunction

        class WebhookMapper(MapFunction):
            def map(self, value):
                return {
                    "event_time": value.get("timestamp", ""),
                    "data": value.get("data", ""),
                    "metadata": {"source": source["name"]}
                }

        env = self.env
        stream = env.from_collection([
            {"timestamp": "2023-01-01T00:00:00Z", "data": "webhook1"},
            {"timestamp": "2023-01-01T00:01:00Z", "data": "webhook2"}
        ], type_info=Types.MAP(Types.STRING(), Types.STRING()))

        stream = stream.map(WebhookMapper())
        return stream

    def create_cdc_stream(self, source: Dict[str, Any]) -> DataStream:
        """Create CDC data stream (simulated)."""
        logger.info(f"Creating CDC stream: {source['name']}")

        from pyflink.common import Types
        from pyflink.datastream.functions import MapFunction

        class CDCMapper(MapFunction):
            def map(self, value):
                return {
                    "event_time": value.get("event_time", ""),
                    "data": value.get("data", value),
                    "metadata": {
                        "source": source["name"],
                        "table": value.get("table"),
                        "op": value.get("op")
                    }
                }

        env = self.env
        stream = env.from_collection([
            {"event_time": "2023-01-01T00:00:00Z", "data": {"id": 1}, "table": "users", "op": "c"},
            {"event_time": "2023-01-01T00:01:00Z", "data": {"id": 2}, "table": "orders", "op": "u"}
        ], type_info=Types.MAP(Types.STRING(), Types.STRING()))

        stream = stream.map(CDCMapper())
        return stream

    def apply_transforms(self, stream: DataStream, transforms: List[Dict[str, Any]]) -> DataStream:
        """Apply configured transforms to the data stream."""
        logger.info(f"Applying {len(transforms)} transforms")

        for transform in transforms:
            transform_type = transform["type"]

            if transform_type == "field_mapping":
                stream = self.apply_field_mapping(stream, transform)
            elif transform_type == "schema_validation":
                stream = self.apply_schema_validation(stream, transform)
            elif transform_type == "watermark":
                stream = self.apply_watermark(stream, transform)
            else:
                logger.warning(f"Unknown transform type: {transform_type}")

        return stream

    def apply_field_mapping(self, stream: DataStream, transform: Dict[str, Any]) -> DataStream:
        """Apply field mapping transformation."""
        from pyflink.datastream.functions import MapFunction

        class FieldMapper(MapFunction):
            def __init__(self, mapping, drop_unmapped):
                self.mapping = mapping
                self.drop_unmapped = drop_unmapped

            def map(self, value):
                result = {}
                for new_field, old_field in self.mapping.items():
                    if old_field in value:
                        result[new_field] = value[old_field]

                if not self.drop_unmapped:
                    for field, val in value.items():
                        if field not in self.mapping.values():
                            result[field] = val

                return result

        logger.info(f"Applying field mapping: {transform['mapping']}")
        stream = stream.map(FieldMapper(transform["mapping"], transform.get("drop_unmapped", True)))
        return stream

    def apply_schema_validation(self, stream: DataStream, transform: Dict[str, Any]) -> DataStream:
        """Apply schema validation transformation."""
        from pyflink.datastream.functions import FilterFunction

        class SchemaValidator(FilterFunction):
            def __init__(self, schema):
                self.schema = schema

            def filter(self, value):
                # Basic validation - check required fields exist
                required_fields = self.schema.get("required", [])
                return all(field in value for field in required_fields)

        logger.info(f"Applying schema validation: {transform['schema']}")
        stream = stream.filter(SchemaValidator(transform["schema"]))
        return stream

    def apply_watermark(self, stream: DataStream, transform: Dict[str, Any]) -> DataStream:
        """Apply watermark for event time processing."""
        from pyflink.common import Types
        from pyflink.datastream.functions import MapFunction
        from pyflink.datastream import TimeCharacteristic

        # Set time characteristic to event time
        self.env.set_stream_time_characteristic(TimeCharacteristic.EventTime)

        class TimestampExtractor(MapFunction):
            def __init__(self, timestamp_column, max_out_of_orderness):
                self.timestamp_column = timestamp_column
                self.max_out_of_orderness = max_out_of_orderness

            def map(self, value):
                import time
                from pyflink.common.time import Time

                # Extract timestamp (simplified)
                timestamp_str = value.get(self.timestamp_column, "")
                if timestamp_str:
                    # In real implementation, parse actual timestamp
                    timestamp_ms = int(time.time() * 1000)
                else:
                    timestamp_ms = int(time.time() * 1000)

                return {
                    **value,
                    "_timestamp": timestamp_ms,
                    "_watermark_delay": self.max_out_of_orderness
                }

        logger.info(f"Applying watermark: {transform['timestamp_column']}")
        stream = stream.map(TimestampExtractor(
            transform["timestamp_column"],
            transform.get("max_out_of_orderness_ms", 5000)
        ))

        return stream

    def create_sinks(self, stream: DataStream, table_env: StreamTableEnvironment) -> None:
        """Create sinks from the data stream."""
        sinks = self.config["sinks"]

        logger.info(f"Creating {len(sinks)} sinks")

        for sink in sinks:
            self.route_stream_to_sink(stream, sink, table_env, None)

    def route_stream_to_sink(
        self,
        stream: DataStream,
        sink: Dict[str, Any],
        table_env: StreamTableEnvironment,
        pipeline: Optional[Dict[str, Any]]
    ) -> None:
        sink_type = sink.get("type")

        if sink_type == "kafka":
            self.create_kafka_sink(stream, sink, table_env, pipeline)
        elif sink_type == "iceberg":
            self.create_iceberg_sink(stream, sink, table_env)
        elif sink_type == "clickhouse":
            self.create_clickhouse_sink(stream, sink, table_env)
        else:
            logger.warning(f"Unsupported sink type: {sink_type}")

    def create_kafka_sink(
        self,
        stream: DataStream,
        sink: Dict[str, Any],
        table_env: StreamTableEnvironment,
        pipeline: Optional[Dict[str, Any]]
    ) -> None:
        """Create Kafka sink."""
        pipeline_id = pipeline.get("id") if pipeline else None
        logger.info(f"Creating Kafka sink: {sink['name']} (pipeline: {pipeline_id})")

        if sink['name'] not in self._sink_tables_created:
            table_env.execute_sql(f"""
            CREATE TABLE {sink['name']} (
                event_time TIMESTAMP(3),
                data STRING,
                metadata ROW<timestamp TIMESTAMP(3), source STRING>
            ) WITH (
                'connector' = 'kafka',
                'topic' = '{sink['topic']}',
                'properties.bootstrap.servers' = '{sink['bootstrap_servers']}',
                'format' = 'json',
                'sink.partitioner' = 'round-robin'
            )
            """)
            self._sink_tables_created.add(sink['name'])

        # Convert stream to table and insert
        table = table_env.from_data_stream(stream)
        table.execute_insert(sink['name'])

    def create_iceberg_sink(self, stream: DataStream, sink: Dict[str, Any], table_env: StreamTableEnvironment) -> None:
        """Create Iceberg sink."""
        logger.info(f"Creating Iceberg sink: {sink['name']}")

        # Create Iceberg sink table
        table_env.execute_sql(f"""
        CREATE TABLE {sink['name']} (
            event_time TIMESTAMP(3),
            data STRING,
            metadata ROW<timestamp TIMESTAMP(3), source STRING>
        ) WITH (
            'connector' = 'iceberg',
            'catalog-name' = '{sink['catalog_name']}',
            'database' = '{sink['database']}',
            'table' = '{sink['table']}',
            'format' = 'parquet',
            'write.wap.enabled' = 'true'
        )
        """)

        # Convert stream to table and insert
        table = table_env.from_data_stream(stream)
        table.execute_insert(sink['name'])

    def create_clickhouse_sink(self, stream: DataStream, sink: Dict[str, Any], table_env: StreamTableEnvironment) -> None:
        """Create ClickHouse sink."""
        logger.info(f"Creating ClickHouse sink: {sink['name']}")

        # Create ClickHouse sink table
        table_env.execute_sql(f"""
        CREATE TABLE {sink['name']} (
            event_time TIMESTAMP(3),
            data STRING,
            metadata ROW<timestamp TIMESTAMP(3), source STRING>
        ) WITH (
            'connector' = 'clickhouse',
            'url' = 'clickhouse://{sink['host']}:{sink['port']}/{sink['database']}',
            'table-name' = '{sink['table']}',
            'sink.batch-size' = '{sink.get('batch_size', 10000)}'
        )
        """)

        # Convert stream to table and insert
        table = table_env.from_data_stream(stream)
        table.execute_insert(sink['name'])

    def _get_sink_by_name(self, name: Optional[str]) -> Optional[Dict[str, Any]]:
        if not name:
            return None
        for sink in self.config.get("sinks", []):
            if sink.get("name") == name:
                return sink
        return None

    def _get_transforms_for_pipeline(self, pipeline: Dict[str, Any]) -> List[Dict[str, Any]]:
        requested = pipeline.get("transforms", [])
        if not requested:
            return []

        available = {transform["name"]: transform for transform in self.config.get("transforms", [])}
        transforms: List[Dict[str, Any]] = []
        for name in requested:
            transform = available.get(name)
            if transform:
                transforms.append(transform)
            else:
                logger.warning(f"Transform '{name}' referenced by pipeline {pipeline.get('id')} not found")
        return transforms


def main():
    """Main entry point for Flink job execution."""
    parser = argparse.ArgumentParser(description="Execute UIS Flink streaming job")
    parser.add_argument("--job-config", required=True, help="Path to job configuration JSON file")
    parser.add_argument("--dry-run", action="store_true", help="Validate configuration without execution")

    args = parser.parse_args()

    # Load job configuration
    with open(args.job_config, 'r') as f:
        config = json.load(f)

    logger.info(f"Loaded job configuration: {config['job_type']}")

    # Create job instance
    job = FlinkStreamingJob(config)

    # Validate configuration
    errors = job.validate_config()
    if errors:
        logger.error(f"Configuration validation failed: {errors}")
        sys.exit(1)

    if args.dry_run:
        logger.info("Dry run completed - configuration is valid")
        return

    # Execute the job
    success = job.execute()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
