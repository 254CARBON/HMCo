#!/usr/bin/env python3
"""
Sample Spark job implementation for executing UIS-generated micro-batch configurations.

This is a reference implementation showing how to execute the Spark configurations
generated by the UIS compiler.
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, Optional
try:
    from pyspark.sql import SparkSession, DataFrame
    from pyspark.sql.functions import (
        col,
        current_timestamp,
        lit,
        struct,
        to_json,
        from_json
    )
except ModuleNotFoundError:  # pragma: no cover - fallback for environments without PySpark
    SparkSession = Any  # type: ignore
    DataFrame = Any  # type: ignore

    def _missing(*_args, **_kwargs):
        raise ModuleNotFoundError(
            "pyspark is required to execute SparkMicroBatchJob; install pyspark to run jobs."
        )

    col = _missing  # type: ignore
    current_timestamp = _missing  # type: ignore
    lit = _missing  # type: ignore
    struct = _missing  # type: ignore
    to_json = _missing  # type: ignore
    from_json = _missing  # type: ignore
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SparkMicroBatchJob:
    """Executes UIS-generated Spark micro-batch configurations."""

    def __init__(self, job_config: Dict[str, Any]):
        """Initialize with job configuration."""
        self.config = job_config
        self.spark = None

    def create_spark_session(self) -> SparkSession:
        """Create Spark session with configuration from job config."""
        spark_config = self.config["spark_config"]

        builder = SparkSession.builder

        # Apply all Spark configuration
        for key, value in spark_config.items():
            if key.startswith("spark."):
                builder = builder.config(key, value)
            else:
                builder = builder.config(f"spark.{key}", value)

        # Set app name
        app_name = self.config.get("app_name", "uis-microbatch-job")
        builder = builder.appName(app_name)

        self.spark = builder.getOrCreate()
        logger.info(f"Created Spark session: {self.spark.sparkContext.applicationId}")
        return self.spark

    def validate_config(self) -> list:
        """Validate job configuration."""
        errors = []

        if not self.config.get("sources"):
            errors.append("No sources configured")

        if not self.config.get("sinks"):
            errors.append("No sinks configured")

        required_keys = ["job_type", "spark_config"]
        for key in required_keys:
            if key not in self.config:
                errors.append(f"Missing required configuration: {key}")

        return errors

    def execute(self) -> bool:
        """Execute the complete micro-batch pipeline."""
        try:
            logger.info("Starting micro-batch job execution")

            # Create Spark session
            spark = self.create_spark_session()

            # Load data from sources
            df = self.load_sources(spark)

            # Apply transforms
            df = self.apply_transforms(df)

            # Write to sinks
            self.write_sinks(df)

            logger.info("Micro-batch job completed successfully")
            return True

        except Exception as e:
            logger.error(f"Micro-batch job failed: {e}")
            raise
        finally:
            if self.spark:
                self.spark.stop()

    def load_sources(self, spark: SparkSession) -> DataFrame:
        """Load data from configured sources."""
        sources = self.config["sources"]
        if not sources:
            raise ValueError("No sources configured")

        logger.info(f"Loading data from {len(sources)} sources")

        # For now, we'll implement REST API source as an example
        # In a full implementation, this would handle all source types
        source = sources[0]  # Use first source

        if source["type"] == "rest_api":
            return self.load_rest_api_source(spark, source)
        elif source["type"] == "file":
            return self.load_file_source(spark, source)
        elif source["type"] == "kafka":
            return self.load_kafka_source(spark, source)
        else:
            raise ValueError(f"Unsupported source type: {source['type']}")

    def load_rest_api_source(self, spark: SparkSession, source: Dict[str, Any]) -> DataFrame:
        """Load data from REST API source."""
        logger.info(f"Loading REST API data from {source['url']}")

        # In a full implementation, this would make HTTP requests
        # For now, return sample data structure
        sample_data = [
            {"id": 1, "name": "John Doe", "email": "john@example.com"},
            {"id": 2, "name": "Jane Smith", "email": "jane@example.com"}
        ]

        # Create DataFrame from sample data
        df = spark.createDataFrame(sample_data)

        # Add metadata columns
        df = df.withColumn("ingestion_timestamp", current_timestamp())
        df = df.withColumn("source_name", lit(source["name"]))

        logger.info(f"Loaded {df.count()} records from REST API")
        return df

    def load_file_source(self, spark: SparkSession, source: Dict[str, Any]) -> DataFrame:
        """Load data from file source."""
        logger.info(f"Loading file data from {source['path']}")

        # Read files based on format
        format_type = source["format"]
        options = source.get("options", {})

        df = spark.read.format(format_type).options(**options).load(source["path"])

        # Add metadata columns
        df = df.withColumn("ingestion_timestamp", current_timestamp())
        df = df.withColumn("source_name", lit(source["name"]))

        logger.info(f"Loaded {df.count()} records from files")
        return df

    def load_kafka_source(self, spark: SparkSession, source: Dict[str, Any]) -> DataFrame:
        """Load data from Kafka source."""
        logger.info(f"Loading Kafka data from {source['bootstrap_servers']}")

        # Kafka streaming source
        df = spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", source["bootstrap_servers"]) \
            .option("subscribe", ",".join(source["topics"])) \
            .option("startingOffsets", source.get("starting_offsets", "latest")) \
            .load()

        # Parse JSON if needed
        if source.get("format") == "json":
            df = df.select(from_json(col("value").cast("string"), source.get("schema", "*")).alias("data")) \
                   .select("data.*")

        # Add metadata columns
        df = df.withColumn("ingestion_timestamp", current_timestamp())
        df = df.withColumn("source_name", lit(source["name"]))

        return df

    def apply_transforms(self, df: DataFrame) -> DataFrame:
        """Apply configured transforms to the DataFrame."""
        transforms = self.config["transforms"]

        logger.info(f"Applying {len(transforms)} transforms")

        for transform in transforms:
            transform_type = transform["type"]

            if transform_type == "field_mapping":
                df = self.apply_field_mapping(df, transform)
            elif transform_type == "schema_validation":
                df = self.apply_schema_validation(df, transform)
            elif transform_type == "custom_spark":
                df = self.apply_custom_spark_transform(df, transform)
            else:
                logger.warning(f"Unknown transform type: {transform_type}")

        return df

    def apply_field_mapping(self, df: DataFrame, transform: Dict[str, Any]) -> DataFrame:
        """Apply field mapping transformation."""
        mapping = transform["mapping"]
        drop_unmapped = transform.get("drop_unmapped", True)

        logger.info(f"Applying field mapping: {mapping}")

        # Create mapping expressions
        select_exprs = []
        for new_name, old_name in mapping.items():
            select_exprs.append(col(old_name).alias(new_name))

        # Add unmapped columns if not dropping them
        if not drop_unmapped:
            existing_fields = [expr.alias() for expr in select_exprs]
            unmapped_fields = [col(field.name) for field in df.schema.fields
                             if field.name not in mapping.values()]
            select_exprs = existing_fields + unmapped_fields

        df = df.select(*select_exprs)
        logger.info(f"Field mapping applied, new schema: {df.schema.fieldNames()}")
        return df

    def apply_schema_validation(self, df: DataFrame, transform: Dict[str, Any]) -> DataFrame:
        """Apply schema validation transformation."""
        schema = transform["schema"]
        mode = transform.get("mode", "FAILFAST")

        logger.info(f"Applying schema validation in {mode} mode")

        # For now, just validate that required fields exist
        required_fields = schema.get("required", [])
        missing_fields = [field for field in required_fields if field not in df.columns]

        if missing_fields:
            error_msg = f"Missing required fields: {missing_fields}"
            if mode == "FAILFAST":
                raise ValueError(error_msg)
            else:
                logger.warning(error_msg)

        return df

    def apply_custom_spark_transform(self, df: DataFrame, transform: Dict[str, Any]) -> DataFrame:
        """Apply custom Spark SQL transformation."""
        sql_query = transform["sql"]
        parameters = transform.get("parameters", {})

        logger.info(f"Applying custom SQL transform: {sql_query}")

        # Create temporary view
        view_name = f"temp_{transform['name']}"
        df.createOrReplaceTempView(view_name)

        # Execute SQL with parameters
        result_df = self.spark.sql(sql_query)

        # Drop temporary view
        self.spark.catalog.dropTempView(view_name)

        return result_df

    def write_sinks(self, df: DataFrame) -> None:
        """Write DataFrame to configured sinks."""
        sinks = self.config["sinks"]

        logger.info(f"Writing data to {len(sinks)} sinks")

        for sink in sinks:
            sink_type = sink["type"]

            if sink_type == "iceberg":
                self.write_iceberg_sink(df, sink)
            elif sink_type == "clickhouse":
                self.write_clickhouse_sink(df, sink)
            elif sink_type == "kafka":
                self.write_kafka_sink(df, sink)
            elif sink_type == "parquet":
                self.write_parquet_sink(df, sink)
            else:
                logger.warning(f"Unsupported sink type: {sink_type}")

    def write_iceberg_sink(self, df: DataFrame, sink: Dict[str, Any]) -> None:
        """Write to Iceberg sink."""
        table = sink["table"]
        mode = sink["mode"]
        options = dict(sink.get("options", {}))

        logger.info(f"Writing to Iceberg table: {table}")

        # Add partition columns if specified
        partition_spec = options.pop("partitionBy", None)
        partition_cols = None
        if partition_spec:
            partition_cols = [col.strip() for col in partition_spec.split(",") if col.strip()]

        writer = df.write.format("iceberg").mode(mode).option("write.wap.enabled", "true")

        # Add partition by if specified
        if partition_cols:
            writer = writer.partitionBy(*partition_cols)

        # Add other options
        for key, value in options.items():
            writer = writer.option(key, value)

        writer.save(table)
        logger.info(f"Successfully wrote to Iceberg table: {table}")

    def write_clickhouse_sink(self, df: DataFrame, sink: Dict[str, Any]) -> None:
        """Write to ClickHouse sink."""
        host = sink["host"]
        port = sink["port"]
        database = sink["database"]
        table = sink["table"]
        mode = sink["mode"]
        options = dict(sink.get("options", {}))

        logger.info(f"Writing to ClickHouse: {host}:{port}/{database}.{table}")

        # ClickHouse JDBC URL
        url = f"jdbc:clickhouse://{host}:{port}/{database}"

        # Add batch size and other options
        batch_size = options.get("batch_size", 10000)
        writer_options = {
            "url": url,
            "dbtable": table,
            "batchsize": str(batch_size),
            "isolationLevel": "NONE"
        }

        df.write.format("jdbc").options(**writer_options).mode(mode).save()
        logger.info(f"Successfully wrote to ClickHouse table: {database}.{table}")

    def write_kafka_sink(self, df: DataFrame, sink: Dict[str, Any]) -> None:
        """Write to Kafka sink."""
        bootstrap_servers = sink["bootstrap_servers"]
        topic = sink["topic"]
        key_field = sink.get("key_field")
        options = dict(sink.get("options", {}))

        logger.info(f"Writing to Kafka topic: {topic}")

        # Convert DataFrame to Kafka format
        kafka_df = df

        # Set key field if specified
        if key_field:
            kafka_df = kafka_df.withColumn("key", col(key_field).cast("string"))
        else:
            kafka_df = kafka_df.withColumn("key", lit(None).cast("string"))

        # Convert to Kafka format
        kafka_df = kafka_df.select(
            col("key"),
            to_json(struct([col(c) for c in kafka_df.columns if c != "key"])).alias("value")
        )

        # Write to Kafka
        kafka_options = {
            "kafka.bootstrap.servers": bootstrap_servers,
            "topic": topic,
            "checkpointLocation": options.get("checkpointLocation", f"/tmp/kafka-checkpoint-{topic}")
        }

        kafka_df.writeStream \
            .format("kafka") \
            .options(**kafka_options) \
            .outputMode("append") \
            .start() \
            .awaitTermination()

        logger.info(f"Successfully started Kafka streaming to topic: {topic}")

    def write_parquet_sink(self, df: DataFrame, sink: Dict[str, Any]) -> None:
        """Write DataFrame to Parquet files in MinIO/S3."""
        path = sink.get("path")
        if not path:
            raise ValueError("Parquet sink requires an output path")

        mode = sink.get("mode", "append")
        options = dict(sink.get("options", {}))

        logger.info(f"Writing Parquet data to {path} (mode={mode})")

        writer = df.write.mode(mode).format("parquet")

        # Handle partitioning if specified
        partition_spec = options.pop("partitionBy", None)
        if partition_spec:
            if isinstance(partition_spec, str):
                partition_columns = [col.strip() for col in partition_spec.split(",") if col.strip()]
            else:
                partition_columns = list(partition_spec)
            if partition_columns:
                writer = writer.partitionBy(*partition_columns)

        for key, value in options.items():
            writer = writer.option(key, value)

        writer.save(path)
        logger.info(f"Successfully wrote Parquet output to {path}")


def main():
    """Main entry point for Spark job execution."""
    parser = argparse.ArgumentParser(description="Execute UIS Spark micro-batch job")
    parser.add_argument("--job-config", required=True, help="Path to job configuration JSON file")
    parser.add_argument("--dry-run", action="store_true", help="Validate configuration without execution")

    args = parser.parse_args()

    # Load job configuration (support file paths and inline JSON)
    job_config_arg = args.job_config
    if Path(job_config_arg).expanduser().exists():
        with open(Path(job_config_arg).expanduser(), 'r') as f:
            config = json.load(f)
        logger.info(f"Loaded job configuration from file: {job_config_arg}")
    else:
        config = json.loads(job_config_arg)
        logger.info("Loaded job configuration from inline JSON argument")

    logger.info(f"Loaded job configuration: {config['job_type']}")

    # Create job instance
    job = SparkMicroBatchJob(config)

    # Validate configuration
    errors = job.validate_config()
    if errors:
        logger.error(f"Configuration validation failed: {errors}")
        sys.exit(1)

    if args.dry_run:
        logger.info("Dry run completed - configuration is valid")
        return

    # Execute the job
    success = job.execute()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
